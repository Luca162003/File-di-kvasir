{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7430c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a815fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/luca/Desktop/Pitone/File-di-kvasir/Kvasir-mask\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir(\"/home/luca/Desktop/Luca/File-di-kvasir_Daniele/\") #metti come directory il path del progetto, all'interno del quale si trova la cartella kvasir-mask\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "IMAGE_DIR = \"Kvasir-mask/images\"          # All 2500 images\n",
    "MASK_DIR = \"Kvasir-mask/masks\"            # Masks for 2000 polyps\n",
    "JSON_PATH = \"Kvasir-mask/bounding-boxes.json\"  # Bounding boxes\n",
    "OUTPUT_DIR = \"Kvasir-mask/kvasir_yolo_seg_dataset\"\n",
    "MODEL_SIZE = 'm'  # YOLOv11-nano\n",
    "BATCH_SIZE = 16   # Increase for nano\n",
    "EPOCHS = 100\n",
    "IMG_SIZE = 640\n",
    "DATA_YAML = f\"{OUTPUT_DIR}/data.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f5ea1",
   "metadata": {},
   "source": [
    "PART 1: DATASET CONVERTER (SEGMENTATION WITH NEGATIVE SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ef6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirToYOLOSeg:\n",
    "    \"\"\"Convert Kvasir masks + JSON to YOLO segmentation format with healthy images.\"\"\"\n",
    "    MIN_AREA = 200  # Minimum area to consider a contour a valid polyp\n",
    "    MAX_ASPECT_RATIO = 8.0\n",
    "\n",
    "    def __init__(self, image_dir, mask_dir, json_path, output_dir, seed=42):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.json_path = Path(json_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.seed = seed\n",
    "\n",
    "        # Load JSON annotations (for polyp images only)\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        if not isinstance(self.annotations, dict):\n",
    "            raise ValueError(\"Annotations JSON must be a dict keyed by image ID.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_to_polygon(mask):\n",
    "        \n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        valid_polygons = []\n",
    "        \n",
    "        # Calculate the approximation tolerance (epsilon) based on the perimeter\n",
    "        epsilon_multiplier = 0.001\n",
    "\n",
    "        for contour in contours:\n",
    "            perimeter = cv2.arcLength(contour, True)\n",
    "            epsilon = epsilon_multiplier * perimeter\n",
    "            \n",
    "            # Approximate the contour to simplify the polygon\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Calculate metrics for noise filtering\n",
    "            area = cv2.contourArea(approx)\n",
    "            x, y, approx_w, approx_h = cv2.boundingRect(approx)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            aspect_ratio = approx_w / approx_h if approx_h != 0 else KvasirToYOLOSeg.MAX_ASPECT_RATIO + 1\n",
    "            \n",
    "            # 1. Area Check: Filters out contours that are too small (noise)\n",
    "            if area < KvasirToYOLOSeg.MIN_AREA:\n",
    "                continue\n",
    "            \n",
    "            # 2. Filter out contours that are too thin/elongated (e.g., line artifacts)\n",
    "            if aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO or 1/aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO:\n",
    "                continue\n",
    "            \n",
    "            # Append the raw NumPy array coordinates\n",
    "            valid_polygons.append(approx.reshape(-1, 2))\n",
    "            \n",
    "        return valid_polygons if valid_polygons else None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_polygon(polygon, img_width, img_height):\n",
    "        \"\"\"Normalize polygon coordinates to [0, 1].\"\"\"\n",
    "        polygon = polygon.astype(float)\n",
    "        polygon[:, 0] /= img_width\n",
    "        polygon[:, 1] /= img_height\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        polygon = np.clip(polygon, 0.0, 1.0)\n",
    "        return polygon\n",
    "\n",
    "    def prepare_dataset(self, train_split=0.7, val_split=0.2, test_split=0.1):\n",
    "        \"\"\"Prepare segmentation dataset with polyp + healthy images.\"\"\"\n",
    "        \n",
    "        if abs(train_split + val_split + test_split - 1.0) > 1e-8:\n",
    "            raise ValueError(f\"Splits must sum to 1.0, got {train_split + val_split + test_split}\")\n",
    "\n",
    "        # Create directories\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get all images\n",
    "        all_images = list(self.image_dir.glob('*.jpg')) + list(self.image_dir.glob('*.png'))\n",
    "        \n",
    "        # Separate polyp vs healthy images\n",
    "        polyp_images = [img for img in all_images if img.stem in self.annotations]\n",
    "        healthy_images = [img for img in all_images if img.stem not in self.annotations]\n",
    "        \n",
    "        # **FIX: Shuffle BEFORE splitting to prevent data leakage**\n",
    "        rnd = random.Random(self.seed)\n",
    "        rnd.shuffle(polyp_images)\n",
    "        rnd.shuffle(healthy_images)\n",
    "        \n",
    "        def split_list(lst, train_r, val_r):\n",
    "            n = len(lst)\n",
    "            n_train = int(n * train_r)\n",
    "            n_val = int(n * val_r)\n",
    "            return lst[:n_train], lst[n_train:n_train + n_val], lst[n_train + n_val:]\n",
    "\n",
    "        polyp_train, polyp_val, polyp_test = split_list(polyp_images, train_split, val_split)\n",
    "        healthy_train, healthy_val, healthy_test = split_list(healthy_images, train_split, val_split)\n",
    "\n",
    "        splits = {\n",
    "            'train': polyp_train + healthy_train,\n",
    "            'val': polyp_val + healthy_val,\n",
    "            'test': polyp_test + healthy_test\n",
    "        }\n",
    "\n",
    "        # Shuffle combined splits to mix polyp and healthy images\n",
    "        for split_imgs in splits.values():\n",
    "            rnd.shuffle(split_imgs)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DATASET SPLIT\")\n",
    "        print(f\"Train: {len(splits['train'])} ({len(polyp_train)} polyps + {len(healthy_train)} healthy)\")\n",
    "        print(f\"Val:   {len(splits['val'])} ({len(polyp_val)} polyps + {len(healthy_val)} healthy)\")\n",
    "        print(f\"Test:  {len(splits['test'])} ({len(polyp_test)} polyps + {len(healthy_test)} healthy)\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        # Process each split\n",
    "        for split_name, images in splits.items():\n",
    "            print(f\"Processing {split_name} split ({len(images)} images)...\")\n",
    "            self._process_split(images, split_name)\n",
    "\n",
    "        # Create YAML\n",
    "        self._create_yaml()\n",
    "\n",
    "        print(f\"Segmentation dataset created!\")\n",
    "        print(f\"  Output: {self.output_dir.resolve()}\")\n",
    "\n",
    "\n",
    "    def _process_split(self, image_files, split_name):\n",
    "        img_dir = self.output_dir / 'images' / split_name\n",
    "        label_dir = self.output_dir / 'labels' / split_name\n",
    "\n",
    "        for img_path in image_files:\n",
    "            img_id = img_path.stem\n",
    "\n",
    "            # Load image and copy to output directory\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read {img_path}\")\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            shutil.copy(img_path, img_dir / img_path.name)\n",
    "\n",
    "            label_path = label_dir / f\"{img_id}.txt\"\n",
    "\n",
    "            if img_id in self.annotations:\n",
    "                # Polyp image processing\n",
    "                \n",
    "                # Load and prepare mask\n",
    "                mask_path = self.mask_dir / f\"{img_id}.jpg\"\n",
    "                if not mask_path.exists():\n",
    "                    mask_path = self.mask_dir / f\"{img_id}.png\"\n",
    "\n",
    "                if not mask_path.exists():\n",
    "                    print(f\"Warning: Mask not found for {img_id}\")\n",
    "                    continue\n",
    "\n",
    "                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    print(f\"Warning: Could not read mask for {img_id}\")\n",
    "                    continue\n",
    "\n",
    "                if mask.shape[:2] != (h, w):\n",
    "                    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Get the SINGLE clean polygon (from the mask_to_polygon logic)\n",
    "                polygons = self.mask_to_polygon(mask)\n",
    "                if not polygons:\n",
    "                    print(f\"Warning: No valid polygons for {img_id}. SKIPPING image.\")\n",
    "                    continue\n",
    "\n",
    "                with open(label_path, 'w') as f:\n",
    "                        for polygon in polygons:\n",
    "                            norm_poly = self.normalize_polygon(polygon, w, h)\n",
    "                            # Format for YOLO: class_id x1 y1 x2 y2 ...\n",
    "                            coords = ' '.join(f\"{x:.6f} {y:.6f}\" for x, y in norm_poly)\n",
    "                            f.write(f\"0 {coords}\\n\")\n",
    "            else:\n",
    "                # Healthy image ‚Äî create empty label file\n",
    "                label_path.touch()\n",
    "                \n",
    "    def _create_yaml(self):\n",
    "        \"\"\"Create data.yaml for segmentation.\"\"\"\n",
    "        data = {\n",
    "            'path': str(self.output_dir.resolve()),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'nc': 1,\n",
    "            'names': ['polyp']\n",
    "        }\n",
    "\n",
    "        yaml_path = self.output_dir / 'data.yaml'\n",
    "        # NOTE: Assumes 'yaml' library is available\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(data, f, default_flow_style=False)\n",
    "        print(f\"\\n  Created: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dab00",
   "metadata": {},
   "source": [
    "PART 2: TRAINING FUNCTION (YOLOv11-seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8dbcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_seg(data_yaml_path, model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                   batch_size=BATCH_SIZE, workers=4, lr0=1e-4):\n",
    "    \"\"\"Train YOLOv11 Segmentation model.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 0\n",
    "        print(\"Using NVIDIA GPU (CUDA)\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Load YOLOv11-seg model\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')  # YOLOv11 segmentation\n",
    "\n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        name='polyp_segmentation_v11',\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        device=device,\n",
    "        workers=workers,\n",
    "        optimizer='AdamW',\n",
    "        project='Kvasir-mask',\n",
    "        \n",
    "        # Learning rate settings\n",
    "        lr0=lr0,\n",
    "        lrf=0.01,\n",
    "        cos_lr=True,\n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        momentum=0.937,\n",
    "        weight_decay=0.001,\n",
    "        dropout=0.1,\n",
    "        \n",
    "        # Multi-scale training\n",
    "        multi_scale=True,\n",
    "        \n",
    "        # Medical imaging augmentations\n",
    "        mosaic=0.0,          # Disabled for medical\n",
    "        mixup=0.0,           # Light mixup\n",
    "        copy_paste=0.0,      # Copy-paste augmentation\n",
    "        erasing=0.1,         # Random erasing\n",
    "        hsv_h=0.01,          # Minimal hue (preserve color)\n",
    "        hsv_s=0.2,\n",
    "        hsv_v=0.2,\n",
    "        degrees=5.0,\n",
    "        translate=0.05,\n",
    "        scale=0.1,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        shear=1.0,\n",
    "        perspective=0.0001,\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        augment=True,\n",
    "        auto_augment='randaugment',\n",
    "        \n",
    "        # Segmentation specific\n",
    "        mask_ratio=4,\n",
    "        overlap_mask=True\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8c2e7",
   "metadata": {},
   "source": [
    "PART 3: EVALUATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16cec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gt_overlay(img, label_path):\n",
    "    \"\"\"Add ground truth overlay to image (green boxes/masks).\"\"\"\n",
    "    if not label_path.exists():\n",
    "        return img\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    overlay = img.copy()\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5:  # Need at least class + 2 points\n",
    "                continue\n",
    "            \n",
    "            # Parse polygon points\n",
    "            coords = list(map(float, parts[1:]))\n",
    "            points = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                x = int(coords[i] * w)\n",
    "                y = int(coords[i + 1] * h)\n",
    "                points.append([x, y])\n",
    "            \n",
    "            if len(points) >= 3:\n",
    "                # Draw filled polygon (semi-transparent green)\n",
    "                pts = np.array(points, dtype=np.int32)\n",
    "                cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
    "                # Draw polygon outline\n",
    "                cv2.polylines(overlay, [pts], True, (0, 200, 0), 2)\n",
    "    \n",
    "    # Blend with original\n",
    "    alpha = 0.3\n",
    "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def evaluate_model_seg(model_path, data_yaml_path, split_name, conf=0.001, iou=0.5):\n",
    "  \n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    print(f\"\\nRunning validation with Conf={conf}, IoU={iou} on split: {split_name}\")\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path, \n",
    "        split=split_name,\n",
    "        conf=conf,\n",
    "        iou=iou\n",
    "    )\n",
    "\n",
    "    # --- CHECK IF SEGMENTATION METRICS EXIST ---\n",
    "    if not hasattr(metrics, 'seg') or metrics.seg is None:\n",
    "        print(\"\\nERROR: This is not a segmentation model! metrics.seg is not available.\")\n",
    "        print(\"Make sure you're using a YOLO-seg model (e.g., yolo-seg.pt)\")\n",
    "        return metrics\n",
    "\n",
    "    # --- GET YOLO'S REPORTED METRICS ---\n",
    "    # These metrics are at the confidence threshold that gives the *optimal F1-score*\n",
    "    # Box metrics\n",
    "    box_p_yolo = metrics.box.p[0] if hasattr(metrics.box, 'p') and len(metrics.box.p) > 0 else 0.0\n",
    "    box_r_yolo = metrics.box.r[0] if hasattr(metrics.box, 'r') and len(metrics.box.r) > 0 else 0.0\n",
    "    box_map50 = metrics.box.map50 if hasattr(metrics.box, 'map50') else 0.0\n",
    "    box_map = metrics.box.map if hasattr(metrics.box, 'map') else 0.0\n",
    "    \n",
    "    # Mask metrics\n",
    "    mask_p_yolo = metrics.seg.p[0] if hasattr(metrics.seg, 'p') and len(metrics.seg.p) > 0 else 0.0\n",
    "    mask_r_yolo = metrics.seg.r[0] if hasattr(metrics.seg, 'r') and len(metrics.seg.r) > 0 else 0.0\n",
    "    mask_map50 = metrics.seg.map50 if hasattr(metrics.seg, 'map50') else 0.0\n",
    "    mask_map = metrics.seg.map if hasattr(metrics.seg, 'map') else 0.0\n",
    "    \n",
    "    # --- Initialize variables for summary ---\n",
    "    total_gt = 0\n",
    "    total_pred = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    # --- EXTRACT CONFUSION MATRIX ---\n",
    "    # Note: The confusion matrix is calculated at the *specific conf* passed to model.val()\n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        cm = metrics.confusion_matrix.matrix\n",
    "        \n",
    "        print(f\"\\nDEBUG - Confusion Matrix:\")\n",
    "        print(f\"{cm}\")\n",
    "        print(f\"\\nStructure (for single-class detection):\")\n",
    "        print(f\"  Rows = Predicted, Cols = Actual\")\n",
    "        print(f\"  [[TP, FP],   ‚Üê Row 0: Predicted polyps\")\n",
    "        print(f\"   [FN, TN]]   ‚Üê Row 1: Predicted background\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  cm[0,0] = {int(cm[0,0])}: True polyps correctly detected (TP)\")\n",
    "        print(f\"  cm[0,1] = {int(cm[0,1])}: Background predicted as polyp (FP) <--- FALSE POSITIVE\")\n",
    "        print(f\"  cm[1,0] = {int(cm[1,0])}: True polyps missed (FN)            <--- FALSE NEGATIVE\")\n",
    "        print(f\"  cm[1,1] = {int(cm[1,1])}: Background correctly identified (TN)\")\n",
    "        \n",
    "        # Extract TP/FP/FN\n",
    "        TP = int(cm[0, 0])  # Predicted Polyp, Actual Polyp\n",
    "        FP = int(cm[0, 1])  # Predicted Polyp, Actual Background\n",
    "        FN = int(cm[1, 0])  # Predicted Background, Actual Polyp\n",
    "        \n",
    "        total_pred = TP + FP\n",
    "        total_gt = TP + FN\n",
    "        \n",
    "        # Calculate metrics from confusion matrix\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\nCalculated from Confusion Matrix (conf={conf}):\")\n",
    "        print(f\"  Total Ground Truth: {total_gt}\")\n",
    "        print(f\"  Total Predictions:  {total_pred}\")\n",
    "        print(f\"  TP={TP}, FP={FP}, FN={FN}\")\n",
    "        print(f\"  Precision: {precision:.4f} = {TP}/({TP}+{FP})\")\n",
    "        print(f\"  Recall:    {recall:.4f} = {TP}/({TP}+{FN})\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "        \n",
    "        # Sanity check against metrics.confusion_matrix.nt\n",
    "        # nt[0] = number of targets for class 0 (polyps)\n",
    "        if hasattr(metrics.confusion_matrix, 'nt') and len(metrics.confusion_matrix.nt) > 0:\n",
    "             total_gt_yolo = int(metrics.confusion_matrix.nt[0])\n",
    "             if total_gt_yolo != total_gt:\n",
    "                 print(f\"  WARNING: CM Total GT ({total_gt}) != metrics.nt[0] ({total_gt_yolo})\")\n",
    "             else:\n",
    "                 print(f\"  (Total Ground Truth {total_gt} matches YOLO's instance count)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nWARNING: Confusion matrix not available!\")\n",
    "        # Fallback using YOLO's reported metrics (less precise)\n",
    "        precision = box_p_yolo\n",
    "        recall = box_r_yolo\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        # We can't know TP/FP/FN for sure without the CM at this conf\n",
    "        total_gt = 0 # Unknown\n",
    "        \n",
    "\n",
    "    # --- PRINT SUMMARY ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION RESULTS ON {split_name.upper()} SET\")\n",
    "    print(f\"Confidence: {conf} | IoU Threshold: {iou}\")\n",
    "    if total_gt > 0:\n",
    "        print(f\"Total Ground Truth Polyps: {total_gt}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(\"\\n### Box (Detection) Metrics\")\n",
    "    print(f\"YOLO Reported (at optimal F1 conf):\")\n",
    "    print(f\"  Precision: {box_p_yolo:.4f}\")\n",
    "    print(f\"  Recall:    {box_r_yolo:.4f}\")\n",
    "    print(f\"  mAP@0.50:  {box_map50:.4f}\")\n",
    "    print(f\"  mAP@0.50:0.95: {box_map:.4f}\")\n",
    "    \n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        print(f\"\\nFrom Confusion Matrix (at conf={conf}):\")\n",
    "        print(f\"  Total Predicted: {total_pred}\")\n",
    "        print(f\"  TP/FP/FN: {TP}/{FP}/{FN}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n### Mask (Segmentation) Metrics\")\n",
    "    print(f\"YOLO Reported (at optimal F1 conf):\")\n",
    "    print(f\"  Precision: {mask_p_yolo:.4f}\")\n",
    "    print(f\"  Recall:    {mask_r_yolo:.4f}\")\n",
    "    print(f\"  mAP@0.50:  {mask_map50:.4f}\")\n",
    "    print(f\"  mAP@0.50:0.95: {mask_map:.4f}\")\n",
    "    \n",
    "    if hasattr(metrics, 'confusion_matrix') and metrics.confusion_matrix is not None:\n",
    "        print(f\"\\nFrom Confusion Matrix (at conf={conf}):\")\n",
    "        print(f\"  Note: Same TP/FP/FN as box (YOLO uses single confusion matrix)\")\n",
    "        print(f\"  Total Predicted: {total_pred}\")\n",
    "        print(f\"  TP/FP/FN: {TP}/{FP}/{FN}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1-score:  {f1:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def predict_on_all_images_seg(model_path, image_dir, data_yaml_path=None, \n",
    "                               conf_threshold=0.25, save_dir='predictions_seg', iou=0.5):\n",
    "    \"\"\"\n",
    "    Run inference, save predicted images with GT overlay, and compute statistics.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    image_dir = Path(image_dir)\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SETUP PATHS ---\n",
    "    # Assumes standard YOLO dataset structure: .../dataset/images/test, .../dataset/labels/test\n",
    "    label_dir = image_dir.parent.parent / 'labels' / image_dir.name\n",
    "    image_files = sorted(list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\")))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "\n",
    "    # --- INITIALIZE COUNTERS ---\n",
    "    total_gt_polyps = 0\n",
    "    total_pred_polyps = 0\n",
    "    TP_img = FP_img = FN_img = TN_img = 0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"1. RUNNING INFERENCE AND VISUALIZATION ON {len(image_files)} IMAGES (Saving GT Overlay)\")\n",
    "    print(f\"   Conf={conf_threshold}, IoU={iou}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # --- 1. INFERENCE, VISUALIZATION, AND IMAGE-LEVEL COUNTING LOOP ---\n",
    "    for img_path in image_files:\n",
    "        label_path = label_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        # Check ground truth\n",
    "        gt_boxes = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                gt_boxes = [line for line in f if line.strip()]\n",
    "        num_gt = len(gt_boxes)\n",
    "        total_gt_polyps += num_gt\n",
    "\n",
    "        # Run inference (single image)\n",
    "        results = model(str(img_path), conf=conf_threshold, iou=iou, verbose=False)\n",
    "        result = results[0] \n",
    "\n",
    "        img_with_results = result.plot()\n",
    "\n",
    "        img_with_gt_overlay = add_gt_overlay(img_with_results, label_path)\n",
    "        \n",
    "        # 3. Save the final image\n",
    "        output_path = save_dir / f\"pred_gt_{img_path.name}\"\n",
    "        img_to_save = np.ascontiguousarray(img_with_gt_overlay, dtype=np.uint8)\n",
    "        cv2.imwrite(str(output_path), img_to_save)\n",
    "\n",
    "        # Count predictions\n",
    "        num_pred = 0\n",
    "        if result.masks is not None:\n",
    "            num_pred = len(result.masks)\n",
    "        \n",
    "        total_pred_polyps += num_pred\n",
    "        \n",
    "        # Contingency matrix (image-level)\n",
    "        if num_gt > 0 and num_pred > 0:\n",
    "            TP_img += 1\n",
    "        elif num_gt == 0 and num_pred > 0:\n",
    "            FP_img += 1\n",
    "        elif num_gt > 0 and num_pred == 0:\n",
    "            FN_img += 1\n",
    "        elif num_gt == 0 and num_pred == 0:\n",
    "            TN_img += 1\n",
    "            \n",
    "        print(f\"  Processed {img_path.name}: GT={num_gt}, Pred={num_pred}\")\n",
    "\n",
    "    # --- 2. CALCULATE IMAGE-LEVEL METRICS ---\n",
    "    \n",
    "    precision_img = TP_img / (TP_img + FP_img) if (TP_img + FP_img) > 0 else 0.0\n",
    "    recall_img = TP_img / (TP_img + FN_img) if (TP_img + FN_img) > 0 else 0.0\n",
    "    f1_score_img = 2 * precision_img * recall_img / (precision_img + recall_img) if (precision_img + recall_img) > 0 else 0.0\n",
    "\n",
    "    # --- 3. FINAL SUMMARY ---\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"Total images:              {len(image_files)}\")\n",
    "    \n",
    "    print(f\"\\n### Image-Level Metrics (Detection/No-Detection)\")\n",
    "    print(f\"  Description: 'Was *any* polyp found in an image that *had* one?'\")\n",
    "    print(f\"  Images with GT & Pred (TP_img): {TP_img}\")\n",
    "    print(f\"  Images with no GT & Pred (FP_img): {FP_img}\")\n",
    "    print(f\"  Images with GT & no Pred (FN_img): {FN_img}\")\n",
    "    print(f\"  Images with no GT & no Pred (TN_img): {TN_img}\")\n",
    "    print(f\"  ---------------------------------\")\n",
    "    print(f\"  Precision (Image-Level): {precision_img:.4f}\")\n",
    "    print(f\"  Recall (Image-Level):    {recall_img:.4f}\")\n",
    "    print(f\"  F1-score (Image-Level):  {f1_score_img:.4f}\")\n",
    "\n",
    "    print(f\"\\n### Polyp-Level Metrics (Object-by-Object)\")\n",
    "    print(f\"  Description: 'Of all {total_gt_polyps} polyps, how many were found?'\")\n",
    "    print(f\"  (Note: This re-runs validation using model.val() for robust metrics)\")\n",
    "    \n",
    "    metrics = None\n",
    "    if data_yaml_path:\n",
    "\n",
    "        metrics = evaluate_model_seg(\n",
    "            model_path=model_path, \n",
    "            data_yaml_path=data_yaml_path, \n",
    "            split_name=image_dir.name, \n",
    "            conf=conf_threshold, \n",
    "            iou=iou\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNote: 'data_yaml_path' not provided. Cannot compute robust polyp-level metrics.\")\n",
    "\n",
    "    print(f\"\\nPredictions saved with GT overlay to: {save_dir.resolve()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Return the metrics of the mask segmentation\n",
    "    if metrics and hasattr(metrics, 'seg'):\n",
    "        mask_p = metrics.seg.p[0] if len(metrics.seg.p) > 0 else 0.0\n",
    "        mask_r = metrics.seg.r[0] if len(metrics.seg.r) > 0 else 0.0\n",
    "        mask_f1 = 2 * mask_p * mask_r / (mask_p + mask_r) if (mask_p + mask_r) > 0 else 0.0\n",
    "        return mask_p, mask_r, mask_f1, metrics.seg.map50, metrics.seg.map\n",
    "    else:\n",
    "        # Fallback if metrics couldn't be calculated\n",
    "        return None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03825d20",
   "metadata": {},
   "source": [
    "Run the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2da8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: DATASET PREPARATION (SEGMENTATION)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATASET SPLIT\n",
      "Train: 1050 (700 polyps + 350 healthy)\n",
      "Val:   300 (200 polyps + 100 healthy)\n",
      "Test:  150 (100 polyps + 50 healthy)\n",
      "======================================================================\n",
      "\n",
      "Processing train split (1050 images)...\n",
      "Processing val split (300 images)...\n",
      "Processing test split (150 images)...\n",
      "\n",
      "  Created: Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\n",
      "Segmentation dataset created!\n",
      "  Output: /home/luca/Desktop/Luca/File-di-kvasir_Daniele/Kvasir-mask/kvasir_yolo_seg_dataset\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# ========== STEP 1: Dataset Preparation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATASET PREPARATION (SEGMENTATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "converter = KvasirToYOLOSeg(IMAGE_DIR, MASK_DIR, JSON_PATH, OUTPUT_DIR, seed=SEED)\n",
    "converter.prepare_dataset(train_split=0.7, val_split=0.2, test_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(f'yolo11{MODEL_SIZE}-seg.pt')  # Replace with your model path if different\n",
    "\n",
    "search_space = {\n",
    "    \"lr0\": (1e-5, 1e-3),\n",
    "    \"lrf\": (0.01, 0.1),\n",
    "    \"momentum\": (0.6, 0.98),\n",
    "    \"weight_decay\": (0.0, 0.001),\n",
    "    \"dfl\": (1.0, 2.0),\n",
    "    \"hsv_h\": (0.0, 0.02),\n",
    "    \"hsv_s\": (0.0, 0.3),\n",
    "    \"hsv_v\": (0.0, 0.3),\n",
    "    \"degrees\": (0.0, 10.0),\n",
    "    \"translate\": (0.0, 0.1),\n",
    "    \"scale\": (0.0, 0.2),\n",
    "    \"shear\": (0.0, 2.0),\n",
    "    \"perspective\": (0.0, 0.0001),\n",
    "    \"flipud\": (0.0, 1.0),\n",
    "    \"fliplr\": (0.0, 1.0),\n",
    "    \"box\": (3.0, 7.5),     # Box loss weight\n",
    "    \"cls\": (0.2, 2.0) \n",
    "}\n",
    "\n",
    "model.tune(\n",
    "    data=\"Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\",\n",
    "    epochs=10,\n",
    "    iterations=100,\n",
    "    optimizer=\"AdamW\",\n",
    "    space=search_space,\n",
    "    plots=True,\n",
    "    save=True,\n",
    "    val=True,\n",
    "    project=\"Kvasir-mask/tune\"  # ‚úÖ Saves results in ./Kvasir_mask/tune/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "    \"\"\"Search for best_hyperparameters.yaml in tune directory.\"\"\"\n",
    "    tune_dir = Path(\"Kvasir-mask/tune\")\n",
    "    \n",
    "    if not tune_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Search recursively for the file\n",
    "    for yaml_file in tune_dir.rglob(\"best_hyperparameters.yaml\"):\n",
    "        print(f\"Found hyperparameters at: {yaml_file}\")\n",
    "        return yaml_file\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def train_yolo_seg_with_tuned_params(data_yaml_path, best_hyperparameters, \n",
    "                                      model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                                      batch_size=BATCH_SIZE, workers=4):\n",
    "    \"\"\"Train with tuned hyperparameters.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')\n",
    "\n",
    "    # ‚úÖ Merge your fixed settings with tuned parameters\n",
    "    training_args = {\n",
    "        'data': data_yaml_path,\n",
    "        'epochs': epochs,\n",
    "        'imgsz': img_size,\n",
    "        'batch': batch_size,\n",
    "        'name': 'polyp_segmentation_v11_tuned',\n",
    "        'patience': 10,\n",
    "        'save': True,\n",
    "        'device': device,\n",
    "        'workers': workers,\n",
    "        'optimizer': 'AdamW',\n",
    "        'project': 'Kvasir-mask',\n",
    "        \n",
    "        # Fixed settings (always use these)\n",
    "        'multi_scale': True,\n",
    "        'mosaic': 0.0,\n",
    "        'mixup': 0.0,\n",
    "        'copy_paste': 0.0,\n",
    "        'augment': True,\n",
    "        'auto_augment': 'randaugment',\n",
    "        'mask_ratio': 4,\n",
    "        'overlap_mask': True,\n",
    "        \n",
    "        # ‚úÖ Add tuned hyperparameters (these override defaults)\n",
    "        **best_hyperparameters  # This unpacks the dictionary\n",
    "    }\n",
    "\n",
    "    results = model.train(**training_args)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: Train YOLOv11-seg ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAINING YOLOv11-seg MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tuned_params_path = find_best_hyperparameters()\n",
    "\n",
    "if tuned_params_path and tuned_params_path.exists():\n",
    "    print(f\"\\n‚úì Found tuned hyperparameters at: {tuned_params_path}\")\n",
    "    \n",
    "    with open(tuned_params_path, 'r') as f:\n",
    "        best_hyperparameters = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"\\nüìã Loaded Hyperparameters:\")\n",
    "    for key, value in best_hyperparameters.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Train with tuned parameters\n",
    "    model = train_yolo_seg_with_tuned_params(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        best_hyperparameters=best_hyperparameters,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11_tuned/weights/best.pt\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No tuned parameters found, using defaults...\")\n",
    "    model = train_yolo_seg(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr0=1e-4  # Good starting point for nano\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11/weights/best.pt\")\n",
    "\n",
    "print(f\"\\n‚úì Using model: {BEST_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69861181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4B: RUNNING INFERENCE ON ALL TEST IMAGES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "1. RUNNING INFERENCE AND VISUALIZATION ON 150 IMAGES (Saving GT Overlay)\n",
      "   Conf=0.95, IoU=0.5\n",
      "======================================================================\n",
      "\n",
      "  Processed 0017b7c7-90f8-4de2-8723-1d87e5c58317.jpg: GT=3, Pred=0\n",
      "  Processed 0198bc50-169e-456c-a56b-970a7f7d23b7.jpg: GT=1, Pred=0\n",
      "  Processed 01dcc13a-85ab-4d6e-b42b-5d631e076ac2.jpg: GT=1, Pred=0\n",
      "  Processed 02bb84bf-1a2e-476f-a7df-f36072bf51f9.jpg: GT=0, Pred=0\n",
      "  Processed 080f65da-aa22-4663-8923-3f4da73c6dca.jpg: GT=0, Pred=0\n",
      "  Processed 08a43621-f3b8-493a-978d-0c802f1cad82.jpg: GT=1, Pred=0\n",
      "  Processed 0ab26ff5-3161-4a17-bcf4-95663033af0a.jpg: GT=0, Pred=0\n",
      "  Processed 0c91bccd-5b06-48bd-8909-ae453b87e5b4.jpg: GT=1, Pred=0\n",
      "  Processed 0d1aa835-4d9a-40e5-8034-115b15b113df.jpg: GT=1, Pred=0\n",
      "  Processed 0d452625-bd76-4a9e-9c75-42f9604a173c.jpg: GT=0, Pred=0\n",
      "  Processed 0d486f49-7a49-4dc4-89fc-48b381307320.jpg: GT=1, Pred=0\n",
      "  Processed 0e6c8f42-1567-47ed-bee5-30a422ab1240.jpg: GT=1, Pred=0\n",
      "  Processed 0f92c3df-cfaa-489a-bbe1-d89f78724849.jpg: GT=1, Pred=0\n",
      "  Processed 10a19193-67e3-41fa-899b-cd7dc5b64274.jpg: GT=0, Pred=0\n",
      "  Processed 128c324f-5c44-4534-b32b-45b2bd4a672e.jpg: GT=0, Pred=0\n",
      "  Processed 14552594-a205-4c89-a9e9-ee8ee07d4f7e.jpg: GT=1, Pred=0\n",
      "  Processed 147257f5-7401-4f3a-bacb-7c16611c2ef1.jpg: GT=1, Pred=0\n",
      "  Processed 14a24372-c34a-4160-8526-77073d54e47b.jpg: GT=1, Pred=0\n",
      "  Processed 14e5d20e-c798-4ce2-82d0-f1f354c89ea1.jpg: GT=1, Pred=0\n",
      "  Processed 174a9acb-8d5b-49ea-93fc-63899d6f8b54.jpg: GT=1, Pred=0\n",
      "  Processed 18985631-fefe-4fbf-aa2b-03a6e3a720da.jpg: GT=1, Pred=0\n",
      "  Processed 195adde8-f99b-4f01-9dba-a8cd573cd26b.jpg: GT=0, Pred=0\n",
      "  Processed 19d0d3bb-5d6c-4ac4-be99-47b9517c8927.jpg: GT=1, Pred=0\n",
      "  Processed 1a290399-a4a8-4d38-b288-0f3fee1ed551.jpg: GT=0, Pred=0\n",
      "  Processed 1aff9042-7a50-4a75-a82e-f6e2d0fc5d4e.jpg: GT=0, Pred=0\n",
      "  Processed 1ce802d6-ae50-40db-bf54-55612e195525.jpg: GT=0, Pred=0\n",
      "  Processed 2029d77f-86c4-49ee-bfe8-352d7deb171e.jpg: GT=0, Pred=0\n",
      "  Processed 20c9258e-9c6e-41f3-b7e3-7fe5fb1280b8.jpg: GT=0, Pred=0\n",
      "  Processed 22eb758f-c7a3-4ba8-b82c-5efe8e6f627f.jpg: GT=1, Pred=0\n",
      "  Processed 232d682d-1558-41fe-ad27-2056a2347fc8.jpg: GT=0, Pred=0\n",
      "  Processed 23c36bb4-f551-48f2-9c69-6aa77be3d5ef.jpg: GT=2, Pred=0\n",
      "  Processed 245cc5e8-3773-4da3-a5ed-eba72af53b2a.jpg: GT=1, Pred=0\n",
      "  Processed 25b9a7cf-d598-40d3-b086-ffdfbe0faad7.jpg: GT=1, Pred=0\n",
      "  Processed 28056be0-4c9d-4b5a-87bb-4449da330383.jpg: GT=1, Pred=0\n",
      "  Processed 2ac19269-ac7f-4c85-83f2-49f11889121c.jpg: GT=0, Pred=0\n",
      "  Processed 2c28fb94-02ff-45ff-bd5d-155f25fa2b79.jpg: GT=0, Pred=0\n",
      "  Processed 2d73486e-9d3a-4c8f-bb51-5e1df14a52e5.jpg: GT=0, Pred=0\n",
      "  Processed 2dadc75e-8fca-4411-88a0-65a3f1cc92be.jpg: GT=1, Pred=0\n",
      "  Processed 2e313668-d14b-448a-a19f-8afd55a6ceca.jpg: GT=0, Pred=0\n",
      "  Processed 2eaf396b-3f87-41b0-9d01-c207bc9955dd.jpg: GT=1, Pred=0\n",
      "  Processed 342e34d5-c824-4840-9619-f62560ec1c1a.jpg: GT=1, Pred=0\n",
      "  Processed 35597c12-5c6e-43b6-b64f-28dd0e01bd84.jpg: GT=1, Pred=0\n",
      "  Processed 36448361-f38c-486d-a404-8251e657df28.jpg: GT=1, Pred=0\n",
      "  Processed 36a05b2b-4068-4f04-9050-dcf7ec755666.jpg: GT=1, Pred=0\n",
      "  Processed 373742ad-cffa-4413-b997-af9caf8847b8.jpg: GT=1, Pred=1\n",
      "  Processed 375881d8-da54-4608-8cbd-c6e26f064fa1.jpg: GT=1, Pred=1\n",
      "  Processed 3887b40d-71a3-49b8-bc1a-cb0a619ade87.jpg: GT=1, Pred=0\n",
      "  Processed 39137435-22a6-4841-914b-3b37933d228f.jpg: GT=0, Pred=0\n",
      "  Processed 394f1922-d2fc-4d27-9dab-d973143b25b5.jpg: GT=0, Pred=0\n",
      "  Processed 3a5a9b76-7d49-4302-b8f4-fd1b0f3ef457.jpg: GT=0, Pred=0\n",
      "  Processed 3b89e1a9-b7c5-4a26-811e-1884ee83967c.jpg: GT=1, Pred=0\n",
      "  Processed 3e055bf6-58ed-41b7-a02f-26f58cd357a9.jpg: GT=1, Pred=0\n",
      "  Processed 3e67665f-b495-42fb-8ffe-ed173204503d.jpg: GT=1, Pred=0\n",
      "  Processed 419b95af-cd80-4fb5-95d3-55aa3d223a6b.jpg: GT=1, Pred=0\n",
      "  Processed 420e19d8-69d2-4157-a6ca-656206886cde.jpg: GT=0, Pred=0\n",
      "  Processed 44be479a-354c-4a0c-82f0-3d728ce44d43.jpg: GT=1, Pred=0\n",
      "  Processed 454b7d17-08da-4a67-8982-7ed04ead6e2b.jpg: GT=1, Pred=0\n",
      "  Processed 455d45c7-7847-439e-a4a8-461ff2dad4fe.jpg: GT=0, Pred=0\n",
      "  Processed 4b23a4c7-b2d9-423b-ae9b-ba2bb9c2a153.jpg: GT=0, Pred=0\n",
      "  Processed 4c3ba172-a7ed-4e55-a29b-e87f4ad4b0e2.jpg: GT=1, Pred=0\n",
      "  Processed 4c53aa4c-8328-444a-b8db-e0eae6bf4bbb.jpg: GT=0, Pred=0\n",
      "  Processed 4d77be91-cc2f-44a1-930b-1041cc83f1b7.jpg: GT=1, Pred=0\n",
      "  Processed 4f6384a7-a909-4636-a951-95ff156ebf5f.jpg: GT=1, Pred=0\n",
      "  Processed 5092f760-f66c-46dc-96c0-de07bd8a6df4.jpg: GT=1, Pred=0\n",
      "  Processed 53067fc1-44d0-4023-a2a9-af6b31b4c3f8.jpg: GT=0, Pred=0\n",
      "  Processed 581f76d4-9147-44a5-8331-118b379604dc.jpg: GT=1, Pred=0\n",
      "  Processed 5a7a835e-0a56-48f5-b44f-09aee583a73c.jpg: GT=0, Pred=0\n",
      "  Processed 5fcb924c-a3a0-4d6e-afd5-0ae48a021c2b.jpg: GT=1, Pred=0\n",
      "  Processed 60508423-0519-4b55-97a4-e4f2a8446e3f.jpg: GT=1, Pred=0\n",
      "  Processed 6058f55d-a0f6-420d-a72b-6076f92d636a.jpg: GT=0, Pred=0\n",
      "  Processed 6345de1a-944e-43e7-8b34-4368d4bc281d.jpg: GT=2, Pred=0\n",
      "  Processed 665a5e76-2496-46ba-acd5-a472672160d3.jpg: GT=1, Pred=0\n",
      "  Processed 69f450f5-a778-4c71-935a-6450199be399.jpg: GT=0, Pred=0\n",
      "  Processed 6b7a6a4a-9655-41f7-af52-de00ef5d6379.jpg: GT=1, Pred=0\n",
      "  Processed 6bac1a5f-5902-4d90-8d6f-f26182512b74.jpg: GT=1, Pred=0\n",
      "  Processed 6d4e71de-b914-4c3b-9ef8-515d2a301948.jpg: GT=1, Pred=0\n",
      "  Processed 6e65bb10-5bad-43a5-a2b5-35d1888f6c57.jpg: GT=0, Pred=0\n",
      "  Processed 6eac2c2e-8e96-4a52-8754-119bb66f01c6.jpg: GT=0, Pred=0\n",
      "  Processed 70cdd8ae-3c62-4331-8f2b-8cda69989b1b.jpg: GT=0, Pred=0\n",
      "  Processed 71f5ea21-9d8f-4855-b21f-ffa5a2b2276b.jpg: GT=1, Pred=0\n",
      "  Processed 79ed69d7-d089-4f9e-88c8-9eb587c1f0e1.jpg: GT=0, Pred=0\n",
      "  Processed 7ab30031-6d04-4a53-af01-8b7261d73123.jpg: GT=1, Pred=0\n",
      "  Processed 7e713539-a3e3-4720-95ce-1447206942a1.jpg: GT=1, Pred=0\n",
      "  Processed 811ecda1-bcbf-4f1b-857b-b8bc717e79ba.jpg: GT=0, Pred=0\n",
      "  Processed 87b45f83-ef3e-4bd4-8514-af8c6aa1c466.jpg: GT=1, Pred=0\n",
      "  Processed 8b87f75b-94db-478d-988a-78ada8365069.jpg: GT=1, Pred=0\n",
      "  Processed 8d32270c-d326-48a1-9e75-da92dee0e9f9.jpg: GT=1, Pred=0\n",
      "  Processed 8d59a679-a24d-409b-911f-d35e080bb990.jpg: GT=0, Pred=0\n",
      "  Processed 8e1e6cdd-d3f9-4671-8ce5-7b4124c7f384.jpg: GT=0, Pred=0\n",
      "  Processed 8e5d6f5f-0f0c-4ac9-9a5d-ff3278136ae3.jpg: GT=0, Pred=0\n",
      "  Processed 8ee599b1-7a84-45ff-85d9-31840c504ad6.jpg: GT=0, Pred=0\n",
      "  Processed 8f064558-7f12-41e7-9949-c3a46b77707d.jpg: GT=1, Pred=0\n",
      "  Processed 8fe8a08b-efef-4c71-b95d-f2edc5902f6b.jpg: GT=1, Pred=0\n",
      "  Processed 936e3ebd-d5a0-4637-8c8b-4d5bc3bf7674.jpg: GT=1, Pred=0\n",
      "  Processed 9507e070-3dc1-466e-ac10-aeb0e7c95edd.jpg: GT=0, Pred=0\n",
      "  Processed 9702b097-4548-400e-bfe3-3aa1d8cfb7a0.jpg: GT=0, Pred=0\n",
      "  Processed 98b29f14-3d39-4c69-885a-770cd29c2125.jpg: GT=0, Pred=0\n",
      "  Processed 9a02753d-763e-4bc2-912b-6396b9fad094.jpg: GT=1, Pred=0\n",
      "  Processed 9bd503e3-13e8-43a2-ac3a-7bebb3be94a0.jpg: GT=1, Pred=0\n",
      "  Processed 9d1f7651-ba26-4b14-bf5c-2b8eb7be97c2.jpg: GT=1, Pred=0\n",
      "  Processed 9ddf9093-1f00-4135-9a16-5b9c9f5216e8.jpg: GT=0, Pred=0\n",
      "  Processed 9e9dab54-5a33-4225-bbf2-e1af57d7b58c.jpg: GT=1, Pred=0\n",
      "  Processed a03be693-637a-458e-9afe-fdc1c0619b46.jpg: GT=1, Pred=0\n",
      "  Processed a1778e38-82f6-4ef7-95f3-af7e47b0655e.jpg: GT=0, Pred=0\n",
      "  Processed a51ac5e3-db34-42d3-a134-d7093d5cd440.jpg: GT=0, Pred=0\n",
      "  Processed a5fcd40e-6a38-49b3-9e09-e34d11897b0c.jpg: GT=0, Pred=0\n",
      "  Processed a82f3176-8ab5-4412-947e-d88051b78e9a.jpg: GT=1, Pred=0\n",
      "  Processed a91ee1a9-4cd3-42c3-87cd-e0b232378cc4.jpg: GT=1, Pred=0\n",
      "  Processed a98591be-f75c-41eb-883b-286ce8ec1b97.jpg: GT=1, Pred=0\n",
      "  Processed aa10ac8d-f9e0-4065-9acc-9cda45093162.jpg: GT=1, Pred=0\n",
      "  Processed ac61069a-7b61-4cf8-b2e4-20395b717f0d.jpg: GT=1, Pred=0\n",
      "  Processed b1e16de3-dda7-403e-a86a-747104779f44.jpg: GT=1, Pred=0\n",
      "  Processed b90b4be4-3c67-45e3-9bf5-c42190b0aa46.jpg: GT=1, Pred=0\n",
      "  Processed b9277dd6-b96c-4458-9069-739b38e3df12.jpg: GT=1, Pred=0\n",
      "  Processed b9613c4d-f5db-42c2-9563-4a6287f946b2.jpg: GT=1, Pred=0\n",
      "  Processed ba35137b-7968-49ab-bc0c-e5ecc39391ee.jpg: GT=1, Pred=0\n",
      "  Processed bb779706-a5a2-4edb-8a42-594b5cf85431.jpg: GT=1, Pred=0\n",
      "  Processed bbc3c151-a3dc-4f75-9b4a-75dba1d19e66.jpg: GT=1, Pred=0\n",
      "  Processed bed5109f-6f19-4119-8479-b2f9a1d28e32.jpg: GT=1, Pred=0\n",
      "  Processed c2ca0a06-d596-4cf4-839c-911e6399a983.jpg: GT=1, Pred=0\n",
      "  Processed c2ea60b7-a93a-4047-949a-31b1109de3b0.jpg: GT=0, Pred=0\n",
      "  Processed c37f32d8-5749-4675-a474-10b1325cd438.jpg: GT=1, Pred=0\n",
      "  Processed c4ceaa9d-e09c-4e2a-918d-721dbc587579.jpg: GT=1, Pred=0\n",
      "  Processed c5455107-cbde-4847-9c8b-681fbcfe9f3d.jpg: GT=1, Pred=1\n",
      "  Processed c7284a44-8c86-4234-afc3-9086c1dba699.jpg: GT=0, Pred=0\n",
      "  Processed c8f58f63-0e15-4bb2-a735-85de69960ed7.jpg: GT=1, Pred=0\n",
      "  Processed c9d308b5-41a2-47eb-9d92-2816658eaf29.jpg: GT=1, Pred=0\n",
      "  Processed cc125ee8-a29e-45ab-9e41-37f9a8b985b2.jpg: GT=1, Pred=0\n",
      "  Processed ccf6122c-dc87-4773-a80d-decc87a56c33.jpg: GT=1, Pred=0\n",
      "  Processed cf066780-f87b-4c54-b3d3-ebe7a30ec849.jpg: GT=0, Pred=0\n",
      "  Processed d05ef2f2-603d-432f-b7fb-edf0101967bd.jpg: GT=1, Pred=0\n",
      "  Processed d4f802ea-a587-4b96-840f-5f6081d5fdbb.jpg: GT=1, Pred=0\n",
      "  Processed d646c5b1-7bbe-4bf6-82b0-10a51abac965.jpg: GT=1, Pred=0\n",
      "  Processed db6cd8c3-9675-4360-ba2c-407dd8c6690f.jpg: GT=1, Pred=0\n",
      "  Processed dc195448-2919-45b3-8b74-119ff48549f3.jpg: GT=0, Pred=0\n",
      "  Processed dd020fb3-f323-40a5-b9d6-02520eae57d0.jpg: GT=1, Pred=0\n",
      "  Processed e09fe35e-e230-4162-b590-ef93b6463241.jpg: GT=1, Pred=0\n",
      "  Processed e3102083-224a-4d1d-acee-9c67901c3c78.jpg: GT=1, Pred=1\n",
      "  Processed e636561e-4119-48be-b0ba-1daa1870d39e.jpg: GT=1, Pred=0\n",
      "  Processed e667ee83-4435-42c6-b69c-c9618dda2aa5.jpg: GT=0, Pred=0\n",
      "  Processed e739e944-9c90-4cca-89b5-1ba0574bc756.jpg: GT=1, Pred=0\n",
      "  Processed e8a4ffdc-4d9d-4aef-80c0-a599615ac53d.jpg: GT=0, Pred=0\n",
      "  Processed f1aaa858-4e16-40b7-98b9-0a9b73c29a7a.jpg: GT=1, Pred=0\n",
      "  Processed f2bdf89f-5ae2-44f4-b417-b791068b72ab.jpg: GT=1, Pred=0\n",
      "  Processed f64f91d4-7db3-4f55-a32c-42fac9e8c171.jpg: GT=2, Pred=0\n",
      "  Processed f845295e-6b54-4677-bc6b-3b7756d67106.jpg: GT=1, Pred=0\n",
      "  Processed f87529af-2036-48c2-a03f-f1cdbdfb3248.jpg: GT=1, Pred=0\n",
      "  Processed f92e56fe-6d79-4f9e-9495-e637b6b99ddb.jpg: GT=1, Pred=0\n",
      "  Processed f93bbf16-9eaf-43e3-83ca-e4eea68995ca.jpg: GT=1, Pred=0\n",
      "  Processed fca927d3-55d2-4be7-9f45-7cb70ca41d1d.jpg: GT=1, Pred=0\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Total images:              150\n",
      "\n",
      "### Image-Level Metrics (Detection/No-Detection)\n",
      "  Description: 'Was *any* polyp found in an image that *had* one?'\n",
      "  Images with GT & Pred (TP_img): 4\n",
      "  Images with no GT & Pred (FP_img): 0\n",
      "  Images with GT & no Pred (FN_img): 96\n",
      "  Images with no GT & no Pred (TN_img): 50\n",
      "  ---------------------------------\n",
      "  Precision (Image-Level): 1.0000\n",
      "  Recall (Image-Level):    0.0400\n",
      "  F1-score (Image-Level):  0.0769\n",
      "\n",
      "### Polyp-Level Metrics (Object-by-Object)\n",
      "  Description: 'Of all 105 polyps, how many were found?'\n",
      "  (Note: This re-runs validation using model.val() for robust metrics)\n",
      "\n",
      "Running validation with Conf=0.95, IoU=0.5 on split: test\n",
      "Ultralytics 8.3.223 üöÄ Python-3.12.3 torch-2.9.0+cu128 CPU (AMD Ryzen 5 5600X 6-Core Processor)\n",
      "YOLO11m-seg summary (fused): 138 layers, 22,336,083 parameters, 0 gradients, 112.9 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2603.7¬±1681.5 MB/s, size: 67.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/luca/Desktop/Luca/File-di-kvasir_Daniele/Kvasir-mask/kvasir_yolo_seg_dataset/labels/test.cache... 150 images, 50 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 150/150 573.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10/10 0.2it/s 1:06.6s1s\n",
      "                   all        150        105          1     0.0762      0.538      0.538          1     0.0762      0.538      0.524\n",
      "Speed: 1.7ms preprocess, 431.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/luca/Desktop/Luca/File-di-kvasir_Daniele/runs/segment/val23\u001b[0m\n",
      "\n",
      "DEBUG - Confusion Matrix:\n",
      "[[          8           0]\n",
      " [         97           0]]\n",
      "\n",
      "Structure (for single-class detection):\n",
      "  Rows = Predicted, Cols = Actual\n",
      "  [[TP, FP],   ‚Üê Row 0: Predicted polyps\n",
      "   [FN, TN]]   ‚Üê Row 1: Predicted background\n",
      "\n",
      "Interpretation:\n",
      "  cm[0,0] = 8: True polyps correctly detected (TP)\n",
      "  cm[0,1] = 0: Background predicted as polyp (FP) <--- FALSE POSITIVE\n",
      "  cm[1,0] = 97: True polyps missed (FN)            <--- FALSE NEGATIVE\n",
      "  cm[1,1] = 0: Background correctly identified (TN)\n",
      "\n",
      "Calculated from Confusion Matrix (conf=0.95):\n",
      "  Total Ground Truth: 105\n",
      "  Total Predictions:  8\n",
      "  TP=8, FP=0, FN=97\n",
      "  Precision: 1.0000 = 8/(8+0)\n",
      "  Recall:    0.0762 = 8/(8+97)\n",
      "  F1-score:  0.1416\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS ON TEST SET\n",
      "Confidence: 0.95 | IoU Threshold: 0.5\n",
      "Total Ground Truth Polyps: 105\n",
      "======================================================================\n",
      "\n",
      "### Box (Detection) Metrics\n",
      "YOLO Reported (at optimal F1 conf):\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0762\n",
      "  mAP@0.50:  0.5381\n",
      "  mAP@0.50:0.95: 0.5381\n",
      "\n",
      "From Confusion Matrix (at conf=0.95):\n",
      "  Total Predicted: 8\n",
      "  TP/FP/FN: 8/0/97\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0762\n",
      "  F1-score:  0.1416\n",
      "\n",
      "### Mask (Segmentation) Metrics\n",
      "YOLO Reported (at optimal F1 conf):\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0762\n",
      "  mAP@0.50:  0.5381\n",
      "  mAP@0.50:0.95: 0.5239\n",
      "\n",
      "From Confusion Matrix (at conf=0.95):\n",
      "  Note: Same TP/FP/FN as box (YOLO uses single confusion matrix)\n",
      "  Total Predicted: 8\n",
      "  TP/FP/FN: 8/0/97\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0762\n",
      "  F1-score:  0.1416\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Predictions saved with GT overlay to: /home/luca/Desktop/Luca/File-di-kvasir_Daniele/test_predictions_seg\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11_tuned/weights/best.pt\")\n",
    "\n",
    "# ========== Full Test Set Evaluation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4B: RUNNING INFERENCE ON ALL TEST IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predict_on_all_images_seg(\n",
    "    BEST_MODEL_PATH, \n",
    "    f\"{OUTPUT_DIR}/images/test\",\n",
    "    data_yaml_path=DATA_YAML,\n",
    "    conf_threshold=0.95,\n",
    "    iou=0.5,#increase to be more lax\n",
    "    save_dir='test_predictions_seg'\n",
    ")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

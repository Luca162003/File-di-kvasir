{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7430c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a815fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir(\"/Users/lucatognari/Pitone/File-di-python/File-di-kvasir\") #metti come directory il path del progetto, all'interno del quale si trova la cartella kvasir-mask\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "IMAGE_DIR = \"Kvasir-mask/images\"          # All 2500 images\n",
    "MASK_DIR = \"Kvasir-mask/masks\"            # Masks for 2000 polyps\n",
    "JSON_PATH = \"Kvasir-mask/bounding-boxes.json\"  # Bounding boxes\n",
    "OUTPUT_DIR = \"Kvasir-mask/kvasir_yolo_seg_dataset\"\n",
    "MODEL_SIZE = 'm'  # YOLOv11-nano\n",
    "BATCH_SIZE = 16   # Increase for nano\n",
    "EPOCHS = 100\n",
    "IMG_SIZE = 640\n",
    "DATA_YAML = f\"{OUTPUT_DIR}/data.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e75962c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 1000 image entries...\n",
      "\n",
      "========================================\n",
      "Detailed Polyp Count and Image ID Summary\n",
      "========================================\n",
      "Total Images Analyzed: 1000\n",
      "Images with 0 polyp:   0\n",
      "Images with 1 polyp:   952\n",
      "Images with >1 polyp:  48\n",
      "\n",
      "--- Image IDs by Specific Polyp Count (Count > 1) ---\n",
      "\n",
      "Images with 2 polyp(s) (41 images):\n",
      "- 90d30949-a733-4e0e-8fdc-b8687e8548ad\n",
      "- 3bd30288-3126-4091-8f5b-b459cde5e48a\n",
      "- 9a8bdb7c-bfa7-4048-a342-4b98d457660c\n",
      "- 2d8b35e2-06c8-4d23-89dd-b97d1ab9283d\n",
      "- a810b859-77c8-4d69-b53b-12d61c2eb5e2\n",
      "- 6345de1a-944e-43e7-8b34-4368d4bc281d\n",
      "- d2d6e4a5-77e0-4859-9376-e9a80d10839c\n",
      "- 892221e8-dbca-4abf-bd54-91da857b3c32\n",
      "- ba1162cf-5c78-46b9-a334-d669f8878b06\n",
      "- 099b142c-4abb-47e6-be7d-230aa7c19fa1\n",
      "- 9564fe16-484f-4af2-b61f-22bf6e01fa8c\n",
      "- fdcde2d9-f531-403f-9bcd-c6256a1e4c78\n",
      "- 6d54bb62-e9c7-4a21-a98b-b95917dd7827\n",
      "- 8fc70ff3-f538-49ab-b153-e3315e11c913\n",
      "- 23c36bb4-f551-48f2-9c69-6aa77be3d5ef\n",
      "- adc070d1-04c3-46b3-8683-a816effcd13e\n",
      "- d4b7902e-046e-4739-899c-af440e33be82\n",
      "- 98d1679d-ff59-4aad-9f69-2e1780de7de6\n",
      "- 605b01a7-9e3f-4b03-8ac1-43fe627c4c82\n",
      "- ff3ba32a-fcb3-40e4-b2d4-b54507dbb677\n",
      "- ae028993-a67c-4413-9f47-e8ab8bc94035\n",
      "- 38e441f1-234b-4da5-8335-994860b9db58\n",
      "- d5b84ca5-5c6a-4bbe-a6ae-e3c4cd5a59a1\n",
      "- ad9ad22d-deb6-40c6-824f-c100bb3c7da1\n",
      "- 1addf8a6-e722-4fe7-b903-1c077c8a48a0\n",
      "- 329f1ad6-1409-46f6-8dc0-9210c9cbc889\n",
      "- a5de17d2-58c9-450f-bcd7-154752fdcfb9\n",
      "- 737f6610-4a35-4092-81a9-2c9af4262954\n",
      "- 25302157-fdd2-4401-a3c7-a31c34f2b928\n",
      "- fd9d587c-98d1-46f8-b8cb-3f9f61931656\n",
      "- e9f9891a-154b-4581-be44-402b7630d323\n",
      "- 2e86a9ba-6d50-4748-8a2b-8dd2c872220b\n",
      "- e441325d-73d6-4a3e-9193-eac98f05f7fa\n",
      "- 0652c533-e7c4-43e5-94ee-23507f6db15c\n",
      "- 312ac427-4fb6-458f-b1bb-06b06ad8b54f\n",
      "- f64f91d4-7db3-4f55-a32c-42fac9e8c171\n",
      "- b3220fae-52ee-493c-aebc-cfd6455be908\n",
      "- 6f583a87-7ed4-415e-a867-2ab3405389ba\n",
      "- d58e96eb-fa26-4b19-93cb-e047a52f587b\n",
      "- a827b9a6-0bb0-4f2a-9f50-2969118981ab\n",
      "- 5252b288-3af4-4182-adb7-423c194bad79\n",
      "\n",
      "Images with 3 polyp(s) (5 images):\n",
      "- 56b48379-588b-4712-a632-6f44a9ec573a\n",
      "- b080aae8-aed9-4610-8af6-27d176574e8b\n",
      "- 241af5c9-884d-4d28-876d-a0b95912bf97\n",
      "- 7659b339-749e-4ce5-9647-15d2bc69e6ed\n",
      "- 0017b7c7-90f8-4de2-8723-1d87e5c58317\n",
      "\n",
      "Images with 4 polyp(s) (1 images):\n",
      "- 7f2c3562-6849-486e-87de-26723b152c07\n",
      "\n",
      "Images with 10 polyp(s) (1 images):\n",
      "- 4604cf0d-7992-48a4-baba-51768bcf0469\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# NOTE: Adjust this path to point to your actual Kvasir annotation JSON file!\n",
    "JSON_PATH = \"Kvasir-mask/bounding-boxes.json\" \n",
    "\n",
    "def count_polyp_images(json_path):\n",
    "    \"\"\"\n",
    "    Analyzes the JSON annotation file to count single and multi-polyp images, \n",
    "    and lists the IDs of images categorized by their exact polyp count.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Annotation file not found at {json_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON file at {json_path}\")\n",
    "        return\n",
    "\n",
    "    # Initialize counters and lists\n",
    "    total_images = len(data)\n",
    "    polyp_counts = defaultdict(int)\n",
    "    # New structure to store IDs by their exact polyp count\n",
    "    polyp_ids_by_count = defaultdict(list) \n",
    "\n",
    "    print(f\"Analyzing {total_images} image entries...\")\n",
    "    \n",
    "    # Iterate through each image key in the JSON file\n",
    "    for image_id, annotation_data in data.items():\n",
    "        \n",
    "        num_polyp = 0\n",
    "        # Check if the annotation contains a list of bounding boxes\n",
    "        if 'bbox' in annotation_data and isinstance(annotation_data['bbox'], list):\n",
    "            num_polyp = len(annotation_data['bbox'])\n",
    "                \n",
    "        polyp_counts[num_polyp] += 1\n",
    "        polyp_ids_by_count[num_polyp].append(image_id) # Store the ID under its count\n",
    "\n",
    "    # --- Summary Printing ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Detailed Polyp Count and Image ID Summary\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Print overall summary\n",
    "    single_polyp_images = polyp_counts.get(1, 0)\n",
    "    zero_polyp_images = polyp_counts.get(0, 0)\n",
    "    multi_polyp_images = sum(count for num, count in polyp_counts.items() if num > 1)\n",
    "\n",
    "    print(f\"Total Images Analyzed: {total_images}\")\n",
    "    print(f\"Images with 0 polyp:   {zero_polyp_images}\")\n",
    "    print(f\"Images with 1 polyp:   {single_polyp_images}\")\n",
    "    print(f\"Images with >1 polyp:  {multi_polyp_images}\")\n",
    "    \n",
    "    # --- Detailed Breakdown of Multi-Polyp Images ---\n",
    "    \n",
    "    # Filter for counts > 1 and sort them for clean printing\n",
    "    multi_polyp_groups = {k: v for k, v in polyp_ids_by_count.items() if k > 1}\n",
    "\n",
    "    if multi_polyp_groups:\n",
    "        print(\"\\n--- Image IDs by Specific Polyp Count (Count > 1) ---\")\n",
    "        \n",
    "        # Print results in ascending order of polyp count\n",
    "        for count in sorted(multi_polyp_groups.keys()):\n",
    "            image_ids = multi_polyp_groups[count]\n",
    "            print(f\"\\nImages with {count} polyp(s) ({len(image_ids)} images):\")\n",
    "            # Print the IDs in a list format\n",
    "            for image_id in image_ids:\n",
    "                print(f\"- {image_id}\")\n",
    "    else:\n",
    "        print(\"\\nNo images found with multiple polyps.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    count_polyp_images(JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f5ea1",
   "metadata": {},
   "source": [
    "PART 1: DATASET CONVERTER (SEGMENTATION WITH NEGATIVE SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08ef6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirToYOLOSeg:\n",
    "    \"\"\"Convert Kvasir masks + JSON to YOLO segmentation format with healthy images.\"\"\"\n",
    "    MIN_AREA = 200  # Minimum area to consider a contour a valid polyp\n",
    "    MAX_ASPECT_RATIO = 8.0\n",
    "\n",
    "    def __init__(self, image_dir, mask_dir, json_path, output_dir, seed=42):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.json_path = Path(json_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.seed = seed\n",
    "\n",
    "        # Load JSON annotations (for polyp images only)\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        if not isinstance(self.annotations, dict):\n",
    "            raise ValueError(\"Annotations JSON must be a dict keyed by image ID.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_to_polygon(mask):\n",
    "        \n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        valid_polygons = []\n",
    "        \n",
    "        # Calculate the approximation tolerance (epsilon) based on the perimeter\n",
    "        epsilon_multiplier = 0.000001\n",
    "\n",
    "        for contour in contours:\n",
    "            perimeter = cv2.arcLength(contour, True)\n",
    "            epsilon = epsilon_multiplier * perimeter\n",
    "            \n",
    "            # Approximate the contour to simplify the polygon\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Calculate metrics for noise filtering\n",
    "            area = cv2.contourArea(approx)\n",
    "            x, y, approx_w, approx_h = cv2.boundingRect(approx)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            aspect_ratio = approx_w / approx_h if approx_h != 0 else KvasirToYOLOSeg.MAX_ASPECT_RATIO + 1\n",
    "            \n",
    "            # 1. Area Check: Filters out contours that are too small (noise)\n",
    "            if area < KvasirToYOLOSeg.MIN_AREA:\n",
    "                continue\n",
    "            \n",
    "            # 2. Filter out contours that are too thin/elongated (e.g., line artifacts)\n",
    "            if aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO or 1/aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO:\n",
    "                continue\n",
    "            \n",
    "            # Append the raw NumPy array coordinates\n",
    "            valid_polygons.append(approx.reshape(-1, 2))\n",
    "            \n",
    "        return valid_polygons if valid_polygons else None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_polygon(polygon, img_width, img_height):\n",
    "        \"\"\"Normalize polygon coordinates to [0, 1].\"\"\"\n",
    "        polygon = polygon.astype(float)\n",
    "        polygon[:, 0] /= img_width\n",
    "        polygon[:, 1] /= img_height\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        polygon = np.clip(polygon, 0.0, 1.0)\n",
    "        return polygon\n",
    "    \n",
    "    # NOTE: _voc_to_yolo_bbox is included but NO LONGER CALLED in _process_split\n",
    "    @staticmethod\n",
    "    def _voc_to_yolo_bbox(bbox, img_width, img_height):\n",
    "        \"\"\"Convert Pascal VOC bbox to YOLO format with validation.\"\"\"\n",
    "        xmin = float(bbox['xmin'])\n",
    "        ymin = float(bbox['ymin'])\n",
    "        xmax = float(bbox['xmax'])\n",
    "        ymax = float(bbox['ymax'])\n",
    "\n",
    "        x_center = (xmin + xmax) / 2.0\n",
    "        y_center = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "\n",
    "        return [0, x_center, y_center, width, height]\n",
    "\n",
    "    def prepare_dataset(self, train_split=0.7, val_split=0.2, test_split=0.1):\n",
    "        \"\"\"Prepare segmentation dataset with polyp + healthy images.\"\"\"\n",
    "        \n",
    "        if abs(train_split + val_split + test_split - 1.0) > 1e-8:\n",
    "            raise ValueError(f\"Splits must sum to 1.0, got {train_split + val_split + test_split}\")\n",
    "\n",
    "        # Create directories\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get all images\n",
    "        all_images = list(self.image_dir.glob('*.jpg')) + list(self.image_dir.glob('*.png'))\n",
    "        \n",
    "        # Separate polyp vs healthy images\n",
    "        polyp_images = [img for img in all_images if img.stem in self.annotations]\n",
    "        healthy_images = [img for img in all_images if img.stem not in self.annotations]\n",
    "\n",
    "        # ... (splitting and shuffling logic remains unchanged) ...\n",
    "        # (omitted for brevity, assume splitting is correct)\n",
    "\n",
    "        # Split each category proportionally\n",
    "        def split_list(lst, train_r, val_r):\n",
    "            n = len(lst)\n",
    "            n_train = int(n * train_r)\n",
    "            n_val = int(n * val_r)\n",
    "            return lst[:n_train], lst[n_train:n_train + n_val], lst[n_train + n_val:]\n",
    "\n",
    "        polyp_train, polyp_val, polyp_test = split_list(polyp_images, train_split, val_split)\n",
    "        healthy_train, healthy_val, healthy_test = split_list(healthy_images, train_split, val_split)\n",
    "\n",
    "        splits = {\n",
    "            'train': polyp_train + healthy_train,\n",
    "            'val': polyp_val + healthy_val,\n",
    "            'test': polyp_test + healthy_test\n",
    "        }\n",
    "\n",
    "        rnd = random.Random(self.seed)\n",
    "        for split_imgs in splits.values():\n",
    "            rnd.shuffle(split_imgs)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DATASET SPLIT\")\n",
    "        # ... (split summary printing remains unchanged) ...\n",
    "        print(f\"Train: {len(splits['train'])} ({len(polyp_train)} polyps + {len(healthy_train)} healthy)\")\n",
    "        print(f\"Val:   {len(splits['val'])} ({len(polyp_val)} polyps + {len(healthy_val)} healthy)\")\n",
    "        print(f\"Test:  {len(splits['test'])} ({len(polyp_test)} polyps + {len(healthy_test)} healthy)\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "        # Process each split\n",
    "        for split_name, images in splits.items():\n",
    "            print(f\"Processing {split_name} split ({len(images)} images)...\")\n",
    "            self._process_split(images, split_name)\n",
    "\n",
    "        # Create YAML\n",
    "        self._create_yaml()\n",
    "\n",
    "        print(f\"\\n✓ Segmentation dataset created!\")\n",
    "        print(f\"  Output: {self.output_dir.resolve()}\")\n",
    "\n",
    "\n",
    "    def _process_split(self, image_files, split_name):\n",
    "        img_dir = self.output_dir / 'images' / split_name\n",
    "        label_dir = self.output_dir / 'labels' / split_name\n",
    "\n",
    "        for img_path in image_files:\n",
    "            img_id = img_path.stem\n",
    "\n",
    "            # Load image and copy to output directory\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read {img_path}\")\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            shutil.copy(img_path, img_dir / img_path.name)\n",
    "\n",
    "            label_path = label_dir / f\"{img_id}.txt\"\n",
    "\n",
    "            if img_id in self.annotations:\n",
    "                # Polyp image processing\n",
    "                \n",
    "                # Load and prepare mask\n",
    "                mask_path = self.mask_dir / f\"{img_id}.jpg\"\n",
    "                if not mask_path.exists():\n",
    "                    mask_path = self.mask_dir / f\"{img_id}.png\"\n",
    "\n",
    "                if not mask_path.exists():\n",
    "                    print(f\"Warning: Mask not found for {img_id}\")\n",
    "                    label_path.touch()\n",
    "                    continue\n",
    "\n",
    "                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    print(f\"Warning: Could not read mask for {img_id}\")\n",
    "                    label_path.touch()\n",
    "                    continue\n",
    "\n",
    "                if mask.shape[:2] != (h, w):\n",
    "                    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Get the SINGLE clean polygon (from the mask_to_polygon logic)\n",
    "                polygons = self.mask_to_polygon(mask)\n",
    "\n",
    "                with open(label_path, 'w') as f:\n",
    "                    # Write segmentation polygon(s)\n",
    "                    if polygons:\n",
    "                        for polygon in polygons:\n",
    "                            norm_poly = self.normalize_polygon(polygon, w, h)\n",
    "                            # Format for YOLO: class_id x1 y1 x2 y2 ...\n",
    "                            coords = ' '.join(f\"{x:.6f} {y:.6f}\" for x, y in norm_poly)\n",
    "                            f.write(f\"0 {coords}\\n\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No valid polygons for {img_id}\")\n",
    "\n",
    "                # !!! CRITICAL FIX: The section that wrote the original BBOX line (0 x y w h) \n",
    "                # has been removed to prevent the redundant second bounding box in the label file.\n",
    "\n",
    "            else:\n",
    "                # Healthy image — create empty label file\n",
    "                label_path.touch()\n",
    "                \n",
    "    def _create_yaml(self):\n",
    "        \"\"\"Create data.yaml for segmentation.\"\"\"\n",
    "        data = {\n",
    "            'path': str(self.output_dir.resolve()),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'nc': 1,\n",
    "            'names': ['polyp']\n",
    "        }\n",
    "\n",
    "        yaml_path = self.output_dir / 'data.yaml'\n",
    "        # NOTE: Assumes 'yaml' library is available\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(data, f, default_flow_style=False)\n",
    "        print(f\"\\n  Created: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FULL DATASET POLYGON DETECTION VERIFICATION\n",
      "======================================================================\n",
      "Starting check on 1000 images with polyps...\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION SUMMARY\n",
      "======================================================================\n",
      "Total Polyp Images Checked: 1000\n",
      "Total Mismatches Found: 0\n",
      "Total Matches Found: 1000\n",
      "\n",
      "**SUCCESS: All detected polygon counts match the expected JSON bounding box counts!**\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You must import your class here! \n",
    "# This assumes your main data preparation class is KvasirToYOLOSeg\n",
    "# and is available in a file named data_prep.py or similar.\n",
    "\n",
    "# --- CONFIGURATION (Adjust these paths!) ---\n",
    "IMAGE_DIR = Path(\"Kvasir-mask/images\") \n",
    "MASK_DIR = Path(\"Kvasir-mask/masks\") \n",
    "JSON_PATH = Path(\"Kvasir-mask/bounding-boxes.json\")\n",
    "\n",
    "# Dummy output path required for KvasirToYOLOSeg initialization\n",
    "DUMMY_OUTPUT_DIR = Path(\"temp_verification_output\") \n",
    "\n",
    "def get_expected_counts(json_path):\n",
    "    \"\"\"Loads the JSON and returns a dictionary of image_id: expected_polyp_count.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "    expected_counts = {}\n",
    "    for img_id, annotation_data in data.items():\n",
    "        num_bbox = 0\n",
    "        if 'bbox' in annotation_data and isinstance(annotation_data['bbox'], list):\n",
    "            num_bbox = len(annotation_data['bbox'])\n",
    "        if num_bbox > 0:\n",
    "             # Only include images with at least one polyp\n",
    "            expected_counts[img_id] = num_bbox\n",
    "            \n",
    "    return expected_counts\n",
    "\n",
    "def run_full_verification(ConverterClass):\n",
    "    \"\"\"\n",
    "    Compares the expected polyp count from JSON bounding boxes with the actual \n",
    "    polygons detected by the ConverterClass's mask_to_polygon method.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"FULL DATASET POLYGON DETECTION VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Get all expected counts from JSON\n",
    "    expected_counts = get_expected_counts(JSON_PATH)\n",
    "    if not expected_counts:\n",
    "        print(\"No polyp annotations found or JSON failed to load. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_polyp_images = len(expected_counts)\n",
    "    mismatch_counts = defaultdict(list)\n",
    "    total_mismatches = 0\n",
    "    \n",
    "    # Initialize the converter class (need to import it first!)\n",
    "    # Note: Since the KvasirToYOLOSeg class is not in this file, you must \n",
    "    # ensure it is imported correctly before running this script.\n",
    "    try:\n",
    "        converter = ConverterClass(IMAGE_DIR, MASK_DIR, JSON_PATH, DUMMY_OUTPUT_DIR)\n",
    "    except NameError:\n",
    "        print(\"CRITICAL ERROR: KvasirToYOLOSeg class is not defined or imported.\")\n",
    "        print(\"Please ensure 'from data_prep import KvasirToYOLOSeg' is uncommented and correct.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing converter: {e}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    print(f\"Starting check on {total_polyp_images} images with polyps...\")\n",
    "\n",
    "    for i, (img_id, expected_count) in enumerate(expected_counts.items()):\n",
    "        \n",
    "        # Determine the mask path\n",
    "        mask_path = MASK_DIR / f\"{img_id}.jpg\"\n",
    "        if not mask_path.exists():\n",
    "            mask_path = MASK_DIR / f\"{img_id}.png\"\n",
    "\n",
    "        if not mask_path.exists():\n",
    "            print(f\"[{i+1}/{total_polyp_images}] WARNING: Mask file not found for {img_id}\")\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess mask\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            print(f\"[{i+1}/{total_polyp_images}] ERROR: Could not read mask for {img_id}\")\n",
    "            continue\n",
    "\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # 2. Get actual detected polygon count\n",
    "        polygons = converter.mask_to_polygon(mask)\n",
    "        detected_count = len(polygons) if polygons else 0\n",
    "        \n",
    "        # 3. Compare and record discrepancies\n",
    "        if detected_count != expected_count:\n",
    "            total_mismatches += 1\n",
    "            mismatch_type = f\"Expected {expected_count} vs Detected {detected_count}\"\n",
    "            mismatch_counts[mismatch_type].append(img_id)\n",
    "            print(f\"[{i+1}/{total_polyp_images}] MISMATCH for {img_id}: Expected {expected_count}, Detected {detected_count}\")\n",
    "        # else:\n",
    "        #     print(f\"[{i+1}/{total_polyp_images}] Match: {img_id} ({expected_count})\") # Uncomment for full logging\n",
    "\n",
    "    # --- Summary Report ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERIFICATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Polyp Images Checked: {total_polyp_images}\")\n",
    "    print(f\"Total Mismatches Found: {total_mismatches}\")\n",
    "    print(f\"Total Matches Found: {total_polyp_images - total_mismatches}\")\n",
    "    \n",
    "    if total_mismatches > 0:\n",
    "        print(\"\\n--- DETAILED MISMATCH REPORT ---\")\n",
    "        for mismatch_type, ids in mismatch_counts.items():\n",
    "            print(f\"\\n{mismatch_type} ({len(ids)} images):\")\n",
    "            for img_id in ids:\n",
    "                print(f\"- {img_id}\")\n",
    "    else:\n",
    "        print(\"\\n**SUCCESS: All detected polygon counts match the expected JSON bounding box counts!**\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: In a Jupyter Notebook, the KvasirToYOLOSeg class should be defined in a previous cell.\n",
    "    # The file-based import is commented out here for notebook compatibility.\n",
    "    # from data_prep import KvasirToYOLOSeg\n",
    "    run_full_verification(KvasirToYOLOSeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of image IDs showing a mismatch (Expected > Detected)\n",
    "PROBLEM_IDS = [\n",
    "    '90d30949-a733-4e0e-8fdc-b8687e8548ad', # Expected 3, Detected 2\n",
    "    '8ea26706-d1f2-441c-93d1-0504fea75f06', # Expected 2, Detected 1\n",
    "    'dc094129-83d2-4f78-a0d0-30fa97012255', # Expected 2, Detected 1\n",
    "    '73d8b58d-6320-43c6-8bec-3ed3e37f9269', # Expected 2, Detected 1\n",
    "    '7e340833-b661-4357-a099-b8cd65c5e3f5', # Expected 2, Detected 1\n",
    "    '101a484a-5a31-493d-97a9-3ec5650c8bb1', # Expected 2, Detected 1\n",
    "    '74138051-ac26-41c5-9bb9-c174f97c8434', # Expected 2, Detected 1\n",
    "    'fff2bc86-e6ad-4ce5-989a-ab63a1c096b3'  # Expected 2, Detected 1\n",
    "]\n",
    "\n",
    "def visualize_annotations(image_id):\n",
    "    \"\"\"\n",
    "    Loads and visualizes the image, mask, and JSON bounding boxes for a single ID.\n",
    "    \"\"\"\n",
    "    img_path = Path(IMAGE_DIR) / f\"{image_id}.jpg\"\n",
    "    mask_path = Path(MASK_DIR) / f\"{image_id}.jpg\"\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        img_path = Path(IMAGE_DIR) / f\"{image_id}.png\"\n",
    "    if not mask_path.exists():\n",
    "        mask_path = Path(MASK_DIR) / f\"{image_id}.png\"\n",
    "\n",
    "    if not img_path.exists():\n",
    "        print(f\"Image not found for ID: {image_id}\")\n",
    "        return\n",
    "\n",
    "    # Load data\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    with open(JSON_PATH, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # --- CRITICAL FIX: Access the nested 'bbox' list based on the user's JSON structure ---\n",
    "    image_data = annotations.get(image_id)\n",
    "    if image_data is None:\n",
    "        print(f\"Warning: No annotation entry found for ID: {image_id}\")\n",
    "        return\n",
    "        \n",
    "    boxes = image_data.get('bbox', [])\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if img is None: return\n",
    "    \n",
    "    # 1. Overlay Mask on Image\n",
    "    \n",
    "    # Resize mask if necessary (Kvasir masks are often 512x512)\n",
    "    if mask is not None and mask.shape[:2] != img.shape[:2]:\n",
    "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert mask to 3 channels for overlay and set transparency\n",
    "    mask_color = np.zeros_like(img, dtype=np.uint8)\n",
    "    mask_color[mask > 127] = [0, 0, 255] # Blue color for mask\n",
    "    \n",
    "    # Blend the mask and image\n",
    "    blended_img = cv2.addWeighted(img, 0.7, mask_color, 0.3, 0)\n",
    "    \n",
    "    # 2. Draw Bounding Boxes\n",
    "    for box_data in boxes:\n",
    "        # box_data is now expected to be a dictionary like {'label': 'polyp', 'xmin': ..., 'ymin': ...}\n",
    "        \n",
    "        # Skip any malformed entries that aren't dictionaries\n",
    "        if not isinstance(box_data, dict):\n",
    "            print(f\"Skipping non-dictionary box data for {image_id}.\")\n",
    "            continue\n",
    "            \n",
    "        # Ensure coordinates are present before accessing\n",
    "        if not all(k in box_data for k in ['xmin', 'ymin', 'xmax', 'ymax']):\n",
    "            print(f\"Skipping box data for {image_id}: Missing coordinate keys.\")\n",
    "            continue\n",
    "            \n",
    "        xmin = int(box_data['xmin'])\n",
    "        ymin = int(box_data['ymin'])\n",
    "        xmax = int(box_data['xmax'])\n",
    "        ymax = int(box_data['ymax'])\n",
    "        \n",
    "        # Draw red bounding box (thickness 3)\n",
    "        cv2.rectangle(blended_img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 3)\n",
    "\n",
    "    # 3. Plotting\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(blended_img)\n",
    "    plt.title(f\"ID: {image_id} | Boxes: {len(boxes)} | Mask Visible: {np.sum(mask>0) > 0}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Starting visualization for problematic IDs...\")\n",
    "print(f\"Total IDs to check: {len(PROBLEM_IDS)}\\n\")\n",
    "\n",
    "for pid in PROBLEM_IDS:\n",
    "    visualize_annotations(pid)\n",
    "\n",
    "print(\"\\nVisualization complete. Review plots to confirm mask-box discrepancy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dab00",
   "metadata": {},
   "source": [
    "PART 2: TRAINING FUNCTION (YOLOv11-seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_seg(data_yaml_path, model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                   batch_size=BATCH_SIZE, workers=4, lr0=1e-4):\n",
    "    \"\"\"Train YOLOv11 Segmentation model.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 0\n",
    "        print(\"Using NVIDIA GPU (CUDA)\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Load YOLOv11-seg model\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')  # YOLOv11 segmentation\n",
    "\n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        name='polyp_segmentation_v11',\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        device=device,\n",
    "        workers=workers,\n",
    "        optimizer='AdamW',\n",
    "        project='Kvasir-mask',\n",
    "        \n",
    "        # Learning rate settings\n",
    "        lr0=lr0,\n",
    "        lrf=0.01,\n",
    "        cos_lr=True,\n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        momentum=0.937,\n",
    "        weight_decay=0.001,\n",
    "        dropout=0.1,\n",
    "        \n",
    "        # Multi-scale training\n",
    "        multi_scale=True,\n",
    "        \n",
    "        # Medical imaging augmentations\n",
    "        mosaic=0.0,          # Disabled for medical\n",
    "        mixup=0.0,           # Light mixup\n",
    "        copy_paste=0.0,      # Copy-paste augmentation\n",
    "        erasing=0.1,         # Random erasing\n",
    "        hsv_h=0.01,          # Minimal hue (preserve color)\n",
    "        hsv_s=0.2,\n",
    "        hsv_v=0.2,\n",
    "        degrees=5.0,\n",
    "        translate=0.05,\n",
    "        scale=0.1,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        shear=1.0,\n",
    "        perspective=0.0001,\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        augment=True,\n",
    "        auto_augment='randaugment',\n",
    "        \n",
    "        # Segmentation specific\n",
    "        mask_ratio=4,\n",
    "        overlap_mask=True\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8c2e7",
   "metadata": {},
   "source": [
    "PART 3: EVALUATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_seg(model_path, data_yaml_path, split='val', conf = 0.001, iou = 0.5):\n",
    "    \"\"\"Evaluate segmentation model.\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    print(f\"Running validation with NMS IOU = {iou} and Conf = {conf}\")\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path, \n",
    "        split=split,\n",
    "        conf=conf,\n",
    "        iou=iou\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION RESULTS ON {split.upper()} SET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Box metrics\n",
    "    print(f\"Box mAP@0.50     : {metrics.box.map50:.4f}\")\n",
    "    print(f\"Box mAP@0.50-95  : {metrics.box.map:.4f}\")\n",
    "    \n",
    "    # Mask metrics (segmentation)\n",
    "    print(f\"\\nMask mAP@0.50    : {metrics.seg.map50:.4f}\")\n",
    "    print(f\"Mask mAP@0.50-95 : {metrics.seg.map:.4f}\")\n",
    "    \n",
    "    if hasattr(metrics.box, 'p') and len(metrics.box.p) > 0:\n",
    "        print(f\"\\nBox Precision    : {metrics.box.p[0]:.4f}\")\n",
    "    if hasattr(metrics.box, 'r') and len(metrics.box.r) > 0:\n",
    "        print(f\"Box Recall       : {metrics.box.r[0]:.4f}\")\n",
    "    \n",
    "    if hasattr(metrics.seg, 'p') and len(metrics.seg.p) > 0:\n",
    "        print(f\"\\nMask Precision   : {metrics.seg.p[0]:.4f}\")\n",
    "    if hasattr(metrics.seg, 'r') and len(metrics.seg.r) > 0:\n",
    "        print(f\"Mask Recall      : {metrics.seg.r[0]:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def predict_and_visualize_seg(model_path, image_path, conf_threshold=0.25, iou = 0.5):\n",
    "    \"\"\"Run inference and visualize segmentation results on a single image.\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    results = model(image_path, conf=conf_threshold, iou=iou)\n",
    "\n",
    "    for result in results:\n",
    "        # Plot with both boxes and masks\n",
    "        img_with_results = result.plot()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(img_with_results, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Polyp Segmentation (confidence > {conf_threshold})', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print detections\n",
    "        if hasattr(result, 'boxes') and len(result.boxes) > 0:\n",
    "            print(f\"\\n✓ Detected {len(result.boxes)} polyp(s):\")\n",
    "            for i, box in enumerate(result.boxes):\n",
    "                conf = float(box.conf[0].item())\n",
    "                print(f\"  Polyp {i+1}: confidence = {conf:.3f}\")\n",
    "        else:\n",
    "            print(\"\\n✓ No polyps detected (healthy image)\")\n",
    "\n",
    "\n",
    "def predict_on_all_images_seg(model_path, image_dir, data_yaml_path=None, \n",
    "                               conf_threshold=0.25, save_dir='predictions_seg', iou = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference on ALL images in a directory, save results, and compute statistics.\n",
    "    Works with segmentation models - saves images with boxes + masks.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to trained segmentation model\n",
    "        image_dir: Directory with test or val images\n",
    "        data_yaml_path: Optional path to data.yaml for mAP evaluation\n",
    "        conf_threshold: Confidence threshold\n",
    "        save_dir: Directory to save prediction images\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    image_dir = Path(image_dir)\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    label_dir = image_dir.parent.parent / 'labels' / image_dir.name\n",
    "    image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RUNNING INFERENCE ON ALL {len(image_files)} IMAGES\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    total_gt_polyps = 0\n",
    "    total_pred_polyps = 0\n",
    "    images_with_polyps = 0\n",
    "\n",
    "    TP = FP = FN = TN = 0\n",
    "\n",
    "    for img_path in image_files:\n",
    "        # Check ground truth\n",
    "        label_path = label_dir / f\"{img_path.stem}.txt\"\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                gt_boxes = [line for line in f if line.strip()]\n",
    "                num_gt = len(gt_boxes)\n",
    "        else:\n",
    "            num_gt = 0\n",
    "\n",
    "        # Run inference\n",
    "        results = model(str(img_path), conf=conf_threshold, iou=iou, verbose=False)\n",
    "\n",
    "        for result in results:\n",
    "            # Save image with boxes AND masks\n",
    "            img_with_results = result.plot()\n",
    "            output_path = save_dir / f\"pred_{img_path.name}\"\n",
    "            img_to_save = np.ascontiguousarray(img_with_results, dtype=np.uint8)\n",
    "            cv2.imwrite(str(output_path), img_with_results)\n",
    "\n",
    "            # Count predictions\n",
    "            if hasattr(result, 'boxes') and len(result.boxes) > 0:\n",
    "                num_pred = len(result.boxes)\n",
    "                total_pred_polyps += num_pred\n",
    "                images_with_polyps += 1\n",
    "                print(f\"✓ {img_path.name}: {num_pred} polyp(s)\")\n",
    "            else:\n",
    "                num_pred = 0\n",
    "                print(f\"  {img_path.name}: No polyps detected\")\n",
    "\n",
    "            # Contingency matrix (image-level)\n",
    "            if num_gt > 0 and num_pred > 0:\n",
    "                TP += 1\n",
    "            elif num_gt == 0 and num_pred > 0:\n",
    "                FP += 1\n",
    "            elif num_gt > 0 and num_pred == 0:\n",
    "                FN += 1\n",
    "            elif num_gt == 0 and num_pred == 0:\n",
    "                TN += 1\n",
    "\n",
    "            total_gt_polyps += num_gt\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # mAP evaluation (uses YOLO's built-in validation)\n",
    "    map50_box = map5095_box = None\n",
    "    map50_mask = map5095_mask = None\n",
    "    \n",
    "    if data_yaml_path:\n",
    "        try:\n",
    "            metrics = model.val(\n",
    "                data=data_yaml_path, \n",
    "                split=image_dir.name, \n",
    "                conf=conf_threshold, \n",
    "                iou=iou,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Box metrics\n",
    "            map50_box = float(getattr(metrics.box, 'map50', 0.0))\n",
    "            map5095_box = float(getattr(metrics.box, 'map', 0.0))\n",
    "            \n",
    "            # Mask metrics (segmentation specific)\n",
    "            if hasattr(metrics, 'seg'):\n",
    "                map50_mask = float(getattr(metrics.seg, 'map50', 0.0))\n",
    "                map5095_mask = float(getattr(metrics.seg, 'map', 0.0))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not compute mAP metrics: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"INFERENCE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total images:              {len(image_files)}\")\n",
    "    print(f\"Images with polyps:        {images_with_polyps}\")\n",
    "    print(f\"Images without polyps:     {len(image_files) - images_with_polyps}\")\n",
    "    print(f\"Total ground truth polyps: {total_gt_polyps}\")\n",
    "    print(f\"Total predicted polyps:    {total_pred_polyps}\")\n",
    "    \n",
    "    print(f\"\\nContingency Matrix (image-level):\")\n",
    "    print(f\"  TP (GT & Pred):           {TP}\")\n",
    "    print(f\"  FP (No GT, Pred):         {FP}\")\n",
    "    print(f\"  FN (GT, No Pred):         {FN}\")\n",
    "    print(f\"  TN (No GT, No Pred):      {TN}\")\n",
    "    \n",
    "    print(f\"\\nImage-Level Metrics:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-score:  {f1_score:.4f}\")\n",
    "    \n",
    "    if map50_box is not None:\n",
    "        print(f\"\\nBox Detection Metrics:\")\n",
    "        print(f\"  mAP@0.50:     {map50_box:.4f}\")\n",
    "        print(f\"  mAP@0.50-0.95: {map5095_box:.4f}\")\n",
    "    \n",
    "    if map50_mask is not None:\n",
    "        print(f\"\\nMask Segmentation Metrics:\")\n",
    "        print(f\"  mAP@0.50:     {map50_mask:.4f}\")\n",
    "        print(f\"  mAP@0.50-0.95: {map5095_mask:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPredictions saved to: {save_dir.resolve()}\")\n",
    "    print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03825d20",
   "metadata": {},
   "source": [
    "Run the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2da8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: DATASET PREPARATION (SEGMENTATION)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATASET SPLIT\n",
      "Train: 1050 (700 polyps + 350 healthy)\n",
      "Val:   300 (200 polyps + 100 healthy)\n",
      "Test:  150 (100 polyps + 50 healthy)\n",
      "======================================================================\n",
      "\n",
      "Processing train split (1050 images)...\n",
      "Processing val split (300 images)...\n",
      "Processing test split (150 images)...\n",
      "\n",
      "  Created: Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\n",
      "\n",
      "✓ Segmentation dataset created!\n",
      "  Output: /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/kvasir_yolo_seg_dataset\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# ========== STEP 1: Dataset Preparation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATASET PREPARATION (SEGMENTATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "converter = KvasirToYOLOSeg(IMAGE_DIR, MASK_DIR, JSON_PATH, OUTPUT_DIR, seed=SEED)\n",
    "converter.prepare_dataset(train_split=0.7, val_split=0.2, test_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8c30da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt to 'yolo11m-seg.pt': 100% ━━━━━━━━━━━━ 43.3MB 19.3MB/s 2.2s2.2s<0.2s\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=/Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/tune/tune'\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m💡 Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/100 with hyperparameters: {'lr0': 0.001, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'dfl': 1.5, 'hsv_h': 0.015, 'hsv_s': 0.3, 'hsv_v': 0.3, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.2, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'box': 7.5, 'cls': 0.5}\n",
      "New https://pypi.org/project/ultralytics/8.3.222 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.221 🚀 Python-3.12.7 torch-2.9.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=Kvasir-mask/tune, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/tune/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.2, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   3718003  ultralytics.nn.modules.head.Segment          [1, 32, 256, [256, 512, 512]] \n",
      "YOLO11m-seg summary: 253 layers, 22,359,987 parameters, 22,359,971 gradients, 113.5 GFLOPs\n",
      "\n",
      "Transferred 705/711 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 238.9±120.1 MB/s, size: 48.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/kvasir_yolo_seg_dataset/labels/train... 1050 images, 350 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1050/1050 4.7Kit/s 0.2s0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/kvasir_yolo_seg_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 236.2±115.8 MB/s, size: 51.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/kvasir_yolo_seg_dataset/labels/val... 300 images, 100 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 300/300 4.8Kit/s 0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/kvasir_yolo_seg_dataset/labels/val.cache\n",
      "Plotting labels to /Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/tune/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 115 weight(decay=0.0), 126 weight(decay=0.0005), 125 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/lucatognari/Pitone/File-di-python/File-di-kvasir/Kvasir-mask/tune/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G     0.7915      2.323      3.585      1.264          8        640: 2% ──────────── 1/66 0.0it/s 1:58<3:37:52"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m model = YOLO(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33myolo11\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-seg.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your model path if different\u001b[39;00m\n\u001b[32m      3\u001b[39m search_space = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr0\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-5\u001b[39m, \u001b[32m1e-3\u001b[39m),\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlrf\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.2\u001b[39m, \u001b[32m2.0\u001b[39m) \n\u001b[32m     21\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKvasir-mask/kvasir_yolo_seg_dataset/data.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKvasir-mask/tune\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Saves results in ./Kvasir_mask/tune/\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pitone/File-di-python/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:855\u001b[39m, in \u001b[36mModel.tune\u001b[39m\u001b[34m(self, use_ray, iterations, *args, **kwargs)\u001b[39m\n\u001b[32m    853\u001b[39m custom = {}  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[32m    854\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pitone/File-di-python/.venv/lib/python3.12/site-packages/ultralytics/engine/tuner.py:397\u001b[39m, in \u001b[36mTuner.__call__\u001b[39m\u001b[34m(self, model, iterations, cleanup)\u001b[39m\n\u001b[32m    395\u001b[39m launch = [\u001b[38;5;28m__import__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msys\u001b[39m\u001b[33m\"\u001b[39m).executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33multralytics.cfg.__init__\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# workaround yolo not found\u001b[39;00m\n\u001b[32m    396\u001b[39m cmd = [*launch, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, *(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_args.items())]\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m return_code = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.returncode\n\u001b[32m    398\u001b[39m ckpt_file = weights_dir / (\u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (weights_dir / \u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlast.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    399\u001b[39m metrics = torch_load(ckpt_file)[\u001b[33m\"\u001b[39m\u001b[33mtrain_metrics\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = YOLO(f'yolo11{MODEL_SIZE}-seg.pt')  # Replace with your model path if different\n",
    "\n",
    "search_space = {\n",
    "    \"lr0\": (1e-5, 1e-3),\n",
    "    \"lrf\": (0.01, 0.1),\n",
    "    \"momentum\": (0.6, 0.98),\n",
    "    \"weight_decay\": (0.0, 0.001),\n",
    "    \"dfl\": (1.0, 2.0),\n",
    "    \"hsv_h\": (0.0, 0.02),\n",
    "    \"hsv_s\": (0.0, 0.3),\n",
    "    \"hsv_v\": (0.0, 0.3),\n",
    "    \"degrees\": (0.0, 10.0),\n",
    "    \"translate\": (0.0, 0.1),\n",
    "    \"scale\": (0.0, 0.2),\n",
    "    \"shear\": (0.0, 2.0),\n",
    "    \"perspective\": (0.0, 0.0001),\n",
    "    \"flipud\": (0.0, 1.0),\n",
    "    \"fliplr\": (0.0, 1.0),\n",
    "    \"box\": (3.0, 7.5),     # Box loss weight\n",
    "    \"cls\": (0.2, 2.0) \n",
    "}\n",
    "\n",
    "model.tune(\n",
    "    data=\"Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\",\n",
    "    epochs=10,\n",
    "    iterations=100,\n",
    "    optimizer=\"AdamW\",\n",
    "    space=search_space,\n",
    "    plots=True,\n",
    "    save=True,\n",
    "    val=True,\n",
    "    project=\"Kvasir-mask/tune\"  # ✅ Saves results in ./Kvasir_mask/tune/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "    \"\"\"Search for best_hyperparameters.yaml in tune directory.\"\"\"\n",
    "    tune_dir = Path(\"Kvasir-mask/tune\")\n",
    "    \n",
    "    if not tune_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Search recursively for the file\n",
    "    for yaml_file in tune_dir.rglob(\"best_hyperparameters.yaml\"):\n",
    "        print(f\"Found hyperparameters at: {yaml_file}\")\n",
    "        return yaml_file\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def train_yolo_seg_with_tuned_params(data_yaml_path, best_hyperparameters, \n",
    "                                      model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                                      batch_size=BATCH_SIZE, workers=4):\n",
    "    \"\"\"Train with tuned hyperparameters.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')\n",
    "\n",
    "    # ✅ Merge your fixed settings with tuned parameters\n",
    "    training_args = {\n",
    "        'data': data_yaml_path,\n",
    "        'epochs': epochs,\n",
    "        'imgsz': img_size,\n",
    "        'batch': batch_size,\n",
    "        'name': 'polyp_segmentation_v11_tuned',\n",
    "        'patience': 10,\n",
    "        'save': True,\n",
    "        'device': device,\n",
    "        'workers': workers,\n",
    "        'optimizer': 'AdamW',\n",
    "        'project': 'Kvasir-mask',\n",
    "        \n",
    "        # Fixed settings (always use these)\n",
    "        'multi_scale': True,\n",
    "        'mosaic': 0.0,\n",
    "        'mixup': 0.0,\n",
    "        'copy_paste': 0.0,\n",
    "        'augment': True,\n",
    "        'auto_augment': 'randaugment',\n",
    "        'mask_ratio': 4,\n",
    "        'overlap_mask': True,\n",
    "        \n",
    "        # ✅ Add tuned hyperparameters (these override defaults)\n",
    "        **best_hyperparameters  # This unpacks the dictionary\n",
    "    }\n",
    "\n",
    "    results = model.train(**training_args)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: Train YOLOv11-seg ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAINING YOLOv11-seg MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tuned_params_path = find_best_hyperparameters()\n",
    "\n",
    "if tuned_params_path and tuned_params_path.exists():\n",
    "    print(f\"\\n✓ Found tuned hyperparameters at: {tuned_params_path}\")\n",
    "    \n",
    "    with open(tuned_params_path, 'r') as f:\n",
    "        best_hyperparameters = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"\\n📋 Loaded Hyperparameters:\")\n",
    "    for key, value in best_hyperparameters.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Train with tuned parameters\n",
    "    model = train_yolo_seg_with_tuned_params(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        best_hyperparameters=best_hyperparameters,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11_tuned/weights/best.pt\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No tuned parameters found, using defaults...\")\n",
    "    model = train_yolo_seg(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr0=1e-4  # Good starting point for nano\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11/weights/best.pt\")\n",
    "\n",
    "print(f\"\\n✓ Using model: {BEST_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 3: Evaluate Model ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "evaluate_model_seg(BEST_MODEL_PATH, DATA_YAML, split='test', conf=0.001, iou=0.5) \n",
    "# ========== STEP 4A: Quick Visual Test ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4A: QUICK VISUAL TEST (Single Image)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_images = list(Path(f\"{OUTPUT_DIR}/images/test\").glob(\"*.*\"))\n",
    "if test_images:\n",
    "    polyp_img = random.choice([img for img in test_images[:10]])\n",
    "    print(f\"\\nTesting on: {polyp_img.name}\")\n",
    "    predict_and_visualize_seg(BEST_MODEL_PATH, str(polyp_img), conf_threshold=0.25) \n",
    "\n",
    "# ========== STEP 4B: Full Test Set Evaluation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4B: RUNNING INFERENCE ON ALL TEST IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predict_on_all_images_seg(\n",
    "    BEST_MODEL_PATH, \n",
    "    f\"{OUTPUT_DIR}/images/test\",\n",
    "    data_yaml_path=DATA_YAML,\n",
    "    conf_threshold=0.25,\n",
    "    iou=0.5,#increase to be more lax\n",
    "    save_dir='test_predictions_seg'\n",
    ")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7430c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a815fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/luca/Desktop/Pitone/File-di-kvasir/Kvasir-mask\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir(\"/home/luca/Desktop/Luca/File-di-kvasir_Daniele/\") #metti come directory il path del progetto, all'interno del quale si trova la cartella kvasir-mask\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "IMAGE_DIR = \"Kvasir-mask/images\"          # All 2500 images\n",
    "MASK_DIR = \"Kvasir-mask/masks\"            # Masks for 2000 polyps\n",
    "JSON_PATH = \"Kvasir-mask/bounding-boxes.json\"  # Bounding boxes\n",
    "OUTPUT_DIR = \"Kvasir-mask/kvasir_yolo_seg_dataset\"\n",
    "MODEL_SIZE = 'm'  # YOLOv11-nano\n",
    "BATCH_SIZE = 16   # Increase for nano\n",
    "EPOCHS = 100\n",
    "IMG_SIZE = 640\n",
    "DATA_YAML = f\"{OUTPUT_DIR}/data.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75962c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# NOTE: Adjust this path to point to your actual Kvasir annotation JSON file!\n",
    "JSON_PATH = \"Kvasir-mask/bounding-boxes.json\" \n",
    "\n",
    "def count_polyp_images(json_path):\n",
    "    \"\"\"\n",
    "    Analyzes the JSON annotation file to count single and multi-polyp images, \n",
    "    and lists the IDs of images categorized by their exact polyp count.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Annotation file not found at {json_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON file at {json_path}\")\n",
    "        return\n",
    "\n",
    "    # Initialize counters and lists\n",
    "    total_images = len(data)\n",
    "    polyp_counts = defaultdict(int)\n",
    "    # New structure to store IDs by their exact polyp count\n",
    "    polyp_ids_by_count = defaultdict(list) \n",
    "\n",
    "    print(f\"Analyzing {total_images} image entries...\")\n",
    "    \n",
    "    # Iterate through each image key in the JSON file\n",
    "    for image_id, annotation_data in data.items():\n",
    "        \n",
    "        num_polyp = 0\n",
    "        # Check if the annotation contains a list of bounding boxes\n",
    "        if 'bbox' in annotation_data and isinstance(annotation_data['bbox'], list):\n",
    "            num_polyp = len(annotation_data['bbox'])\n",
    "                \n",
    "        polyp_counts[num_polyp] += 1\n",
    "        polyp_ids_by_count[num_polyp].append(image_id) # Store the ID under its count\n",
    "\n",
    "    # --- Summary Printing ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Detailed Polyp Count and Image ID Summary\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Print overall summary\n",
    "    single_polyp_images = polyp_counts.get(1, 0)\n",
    "    zero_polyp_images = polyp_counts.get(0, 0)\n",
    "    multi_polyp_images = sum(count for num, count in polyp_counts.items() if num > 1)\n",
    "\n",
    "    print(f\"Total Images Analyzed: {total_images}\")\n",
    "    print(f\"Images with 0 polyp:   {zero_polyp_images}\")\n",
    "    print(f\"Images with 1 polyp:   {single_polyp_images}\")\n",
    "    print(f\"Images with >1 polyp:  {multi_polyp_images}\")\n",
    "    \n",
    "    # --- Detailed Breakdown of Multi-Polyp Images ---\n",
    "    \n",
    "    # Filter for counts > 1 and sort them for clean printing\n",
    "    multi_polyp_groups = {k: v for k, v in polyp_ids_by_count.items() if k > 1}\n",
    "\n",
    "    if multi_polyp_groups:\n",
    "        print(\"\\n--- Image IDs by Specific Polyp Count (Count > 1) ---\")\n",
    "        \n",
    "        # Print results in ascending order of polyp count\n",
    "        for count in sorted(multi_polyp_groups.keys()):\n",
    "            image_ids = multi_polyp_groups[count]\n",
    "            print(f\"\\nImages with {count} polyp(s) ({len(image_ids)} images):\")\n",
    "            # Print the IDs in a list format\n",
    "            for image_id in image_ids:\n",
    "                print(f\"- {image_id}\")\n",
    "    else:\n",
    "        print(\"\\nNo images found with multiple polyps.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    count_polyp_images(JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f5ea1",
   "metadata": {},
   "source": [
    "PART 1: DATASET CONVERTER (SEGMENTATION WITH NEGATIVE SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ef6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirToYOLOSeg:\n",
    "    \"\"\"Convert Kvasir masks + JSON to YOLO segmentation format with healthy images.\"\"\"\n",
    "    MIN_AREA = 200  # Minimum area to consider a contour a valid polyp\n",
    "    MAX_ASPECT_RATIO = 8.0\n",
    "\n",
    "    def __init__(self, image_dir, mask_dir, json_path, output_dir, seed=42):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.json_path = Path(json_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.seed = seed\n",
    "\n",
    "        # Load JSON annotations (for polyp images only)\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        if not isinstance(self.annotations, dict):\n",
    "            raise ValueError(\"Annotations JSON must be a dict keyed by image ID.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_to_polygon(mask):\n",
    "        \n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        valid_polygons = []\n",
    "        \n",
    "        # Calculate the approximation tolerance (epsilon) based on the perimeter\n",
    "        epsilon_multiplier = 0.000001\n",
    "\n",
    "        for contour in contours:\n",
    "            perimeter = cv2.arcLength(contour, True)\n",
    "            epsilon = epsilon_multiplier * perimeter\n",
    "            \n",
    "            # Approximate the contour to simplify the polygon\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Calculate metrics for noise filtering\n",
    "            area = cv2.contourArea(approx)\n",
    "            x, y, approx_w, approx_h = cv2.boundingRect(approx)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            aspect_ratio = approx_w / approx_h if approx_h != 0 else KvasirToYOLOSeg.MAX_ASPECT_RATIO + 1\n",
    "            \n",
    "            # 1. Area Check: Filters out contours that are too small (noise)\n",
    "            if area < KvasirToYOLOSeg.MIN_AREA:\n",
    "                continue\n",
    "            \n",
    "            # 2. Filter out contours that are too thin/elongated (e.g., line artifacts)\n",
    "            if aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO or 1/aspect_ratio > KvasirToYOLOSeg.MAX_ASPECT_RATIO:\n",
    "                continue\n",
    "            \n",
    "            # Append the raw NumPy array coordinates\n",
    "            valid_polygons.append(approx.reshape(-1, 2))\n",
    "            \n",
    "        return valid_polygons if valid_polygons else None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_polygon(polygon, img_width, img_height):\n",
    "        \"\"\"Normalize polygon coordinates to [0, 1].\"\"\"\n",
    "        polygon = polygon.astype(float)\n",
    "        polygon[:, 0] /= img_width\n",
    "        polygon[:, 1] /= img_height\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        polygon = np.clip(polygon, 0.0, 1.0)\n",
    "        return polygon\n",
    "\n",
    "    def prepare_dataset(self, train_split=0.7, val_split=0.2, test_split=0.1):\n",
    "        \"\"\"Prepare segmentation dataset with polyp + healthy images.\"\"\"\n",
    "        \n",
    "        if abs(train_split + val_split + test_split - 1.0) > 1e-8:\n",
    "            raise ValueError(f\"Splits must sum to 1.0, got {train_split + val_split + test_split}\")\n",
    "\n",
    "        # Create directories\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get all images\n",
    "        all_images = list(self.image_dir.glob('*.jpg')) + list(self.image_dir.glob('*.png'))\n",
    "        \n",
    "        # Separate polyp vs healthy images\n",
    "        polyp_images = [img for img in all_images if img.stem in self.annotations]\n",
    "        healthy_images = [img for img in all_images if img.stem not in self.annotations]\n",
    "        def split_list(lst, train_r, val_r):\n",
    "            n = len(lst)\n",
    "            n_train = int(n * train_r)\n",
    "            n_val = int(n * val_r)\n",
    "            return lst[:n_train], lst[n_train:n_train + n_val], lst[n_train + n_val:]\n",
    "\n",
    "        polyp_train, polyp_val, polyp_test = split_list(polyp_images, train_split, val_split)\n",
    "        healthy_train, healthy_val, healthy_test = split_list(healthy_images, train_split, val_split)\n",
    "\n",
    "        splits = {\n",
    "            'train': polyp_train + healthy_train,\n",
    "            'val': polyp_val + healthy_val,\n",
    "            'test': polyp_test + healthy_test\n",
    "        }\n",
    "\n",
    "        rnd = random.Random(self.seed)\n",
    "        for split_imgs in splits.values():\n",
    "            rnd.shuffle(split_imgs)\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DATASET SPLIT\")\n",
    "        # ... (split summary printing remains unchanged) ...\n",
    "        print(f\"Train: {len(splits['train'])} ({len(polyp_train)} polyps + {len(healthy_train)} healthy)\")\n",
    "        print(f\"Val:   {len(splits['val'])} ({len(polyp_val)} polyps + {len(healthy_val)} healthy)\")\n",
    "        print(f\"Test:  {len(splits['test'])} ({len(polyp_test)} polyps + {len(healthy_test)} healthy)\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "        # Process each split\n",
    "        for split_name, images in splits.items():\n",
    "            print(f\"Processing {split_name} split ({len(images)} images)...\")\n",
    "            self._process_split(images, split_name)\n",
    "\n",
    "        # Create YAML\n",
    "        self._create_yaml()\n",
    "\n",
    "        print(f\"Segmentation dataset created!\")\n",
    "        print(f\"  Output: {self.output_dir.resolve()}\")\n",
    "\n",
    "\n",
    "    def _process_split(self, image_files, split_name):\n",
    "        img_dir = self.output_dir / 'images' / split_name\n",
    "        label_dir = self.output_dir / 'labels' / split_name\n",
    "\n",
    "        for img_path in image_files:\n",
    "            img_id = img_path.stem\n",
    "\n",
    "            # Load image and copy to output directory\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read {img_path}\")\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            shutil.copy(img_path, img_dir / img_path.name)\n",
    "\n",
    "            label_path = label_dir / f\"{img_id}.txt\"\n",
    "\n",
    "            if img_id in self.annotations:\n",
    "                # Polyp image processing\n",
    "                \n",
    "                # Load and prepare mask\n",
    "                mask_path = self.mask_dir / f\"{img_id}.jpg\"\n",
    "                if not mask_path.exists():\n",
    "                    mask_path = self.mask_dir / f\"{img_id}.png\"\n",
    "\n",
    "                if not mask_path.exists():\n",
    "                    print(f\"Warning: Mask not found for {img_id}\")\n",
    "                    label_path.touch()\n",
    "                    continue\n",
    "\n",
    "                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    print(f\"Warning: Could not read mask for {img_id}\")\n",
    "                    label_path.touch()\n",
    "                    continue\n",
    "\n",
    "                if mask.shape[:2] != (h, w):\n",
    "                    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Get the SINGLE clean polygon (from the mask_to_polygon logic)\n",
    "                polygons = self.mask_to_polygon(mask)\n",
    "\n",
    "                with open(label_path, 'w') as f:\n",
    "                    # Write segmentation polygon(s)\n",
    "                    if polygons:\n",
    "                        for polygon in polygons:\n",
    "                            norm_poly = self.normalize_polygon(polygon, w, h)\n",
    "                            # Format for YOLO: class_id x1 y1 x2 y2 ...\n",
    "                            coords = ' '.join(f\"{x:.6f} {y:.6f}\" for x, y in norm_poly)\n",
    "                            f.write(f\"0 {coords}\\n\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No valid polygons for {img_id}\")\n",
    "            else:\n",
    "                # Healthy image — create empty label file\n",
    "                label_path.touch()\n",
    "                \n",
    "    def _create_yaml(self):\n",
    "        \"\"\"Create data.yaml for segmentation.\"\"\"\n",
    "        data = {\n",
    "            'path': str(self.output_dir.resolve()),\n",
    "            'train': 'images/train',\n",
    "            'val': 'images/val',\n",
    "            'test': 'images/test',\n",
    "            'nc': 1,\n",
    "            'names': ['polyp']\n",
    "        }\n",
    "\n",
    "        yaml_path = self.output_dir / 'data.yaml'\n",
    "        # NOTE: Assumes 'yaml' library is available\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(data, f, default_flow_style=False)\n",
    "        print(f\"\\n  Created: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION (Adjust these paths!) ---\n",
    "IMAGE_DIR = Path(\"Kvasir-mask/images\") \n",
    "MASK_DIR = Path(\"Kvasir-mask/masks\") \n",
    "JSON_PATH = Path(\"Kvasir-mask/bounding-boxes.json\")\n",
    "\n",
    "# Dummy output path required for KvasirToYOLOSeg initialization\n",
    "DUMMY_OUTPUT_DIR = Path(\"temp_verification_output\") \n",
    "\n",
    "def get_expected_counts(json_path):\n",
    "    \"\"\"Loads the JSON and returns a dictionary of image_id: expected_polyp_count.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "    expected_counts = {}\n",
    "    for img_id, annotation_data in data.items():\n",
    "        num_bbox = 0\n",
    "        if 'bbox' in annotation_data and isinstance(annotation_data['bbox'], list):\n",
    "            num_bbox = len(annotation_data['bbox'])\n",
    "        if num_bbox > 0:\n",
    "             # Only include images with at least one polyp\n",
    "            expected_counts[img_id] = num_bbox\n",
    "            \n",
    "    return expected_counts\n",
    "\n",
    "def run_full_verification(ConverterClass):\n",
    "    \"\"\"\n",
    "    Compares the expected polyp count from JSON bounding boxes with the actual \n",
    "    polygons detected by the ConverterClass's mask_to_polygon method.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"FULL DATASET POLYGON DETECTION VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Get all expected counts from JSON\n",
    "    expected_counts = get_expected_counts(JSON_PATH)\n",
    "    if not expected_counts:\n",
    "        print(\"No polyp annotations found or JSON failed to load. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_polyp_images = len(expected_counts)\n",
    "    mismatch_counts = defaultdict(list)\n",
    "    total_mismatches = 0\n",
    "    \n",
    "    # Initialize the converter class (need to import it first!)\n",
    "    # Note: Since the KvasirToYOLOSeg class is not in this file, you must \n",
    "    # ensure it is imported correctly before running this script.\n",
    "    try:\n",
    "        converter = ConverterClass(IMAGE_DIR, MASK_DIR, JSON_PATH, DUMMY_OUTPUT_DIR)\n",
    "    except NameError:\n",
    "        print(\"CRITICAL ERROR: KvasirToYOLOSeg class is not defined or imported.\")\n",
    "        print(\"Please ensure 'from data_prep import KvasirToYOLOSeg' is uncommented and correct.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing converter: {e}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    print(f\"Starting check on {total_polyp_images} images with polyps...\")\n",
    "\n",
    "    for i, (img_id, expected_count) in enumerate(expected_counts.items()):\n",
    "        \n",
    "        # Determine the mask path\n",
    "        mask_path = MASK_DIR / f\"{img_id}.jpg\"\n",
    "        if not mask_path.exists():\n",
    "            mask_path = MASK_DIR / f\"{img_id}.png\"\n",
    "\n",
    "        if not mask_path.exists():\n",
    "            print(f\"[{i+1}/{total_polyp_images}] WARNING: Mask file not found for {img_id}\")\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess mask\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            print(f\"[{i+1}/{total_polyp_images}] ERROR: Could not read mask for {img_id}\")\n",
    "            continue\n",
    "\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # 2. Get actual detected polygon count\n",
    "        polygons = converter.mask_to_polygon(mask)\n",
    "        detected_count = len(polygons) if polygons else 0\n",
    "        \n",
    "        # 3. Compare and record discrepancies\n",
    "        if detected_count != expected_count:\n",
    "            total_mismatches += 1\n",
    "            mismatch_type = f\"Expected {expected_count} vs Detected {detected_count}\"\n",
    "            mismatch_counts[mismatch_type].append(img_id)\n",
    "            print(f\"[{i+1}/{total_polyp_images}] MISMATCH for {img_id}: Expected {expected_count}, Detected {detected_count}\")\n",
    "        # else:\n",
    "        #     print(f\"[{i+1}/{total_polyp_images}] Match: {img_id} ({expected_count})\") # Uncomment for full logging\n",
    "\n",
    "    # --- Summary Report ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERIFICATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Polyp Images Checked: {total_polyp_images}\")\n",
    "    print(f\"Total Mismatches Found: {total_mismatches}\")\n",
    "    print(f\"Total Matches Found: {total_polyp_images - total_mismatches}\")\n",
    "    \n",
    "    if total_mismatches > 0:\n",
    "        print(\"\\n--- DETAILED MISMATCH REPORT ---\")\n",
    "        for mismatch_type, ids in mismatch_counts.items():\n",
    "            print(f\"\\n{mismatch_type} ({len(ids)} images):\")\n",
    "            for img_id in ids:\n",
    "                print(f\"- {img_id}\")\n",
    "    else:\n",
    "        print(\"\\n**SUCCESS: All detected polygon counts match the expected JSON bounding box counts!**\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: In a Jupyter Notebook, the KvasirToYOLOSeg class should be defined in a previous cell.\n",
    "    # The file-based import is commented out here for notebook compatibility.\n",
    "    # from data_prep import KvasirToYOLOSeg\n",
    "    run_full_verification(KvasirToYOLOSeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "933d2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visualization for problematic IDs...\n",
      "Total IDs to check: 0\n",
      "\n",
      "\n",
      "Visualization complete. Review plots to confirm mask-box discrepancy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of image IDs showing a mismatch (Expected > Detected)\n",
    "PROBLEM_IDS = []\n",
    "\n",
    "def visualize_annotations(image_id):\n",
    "    \"\"\"\n",
    "    Loads and visualizes the image, mask, and JSON bounding boxes for a single ID.\n",
    "    \"\"\"\n",
    "    img_path = Path(IMAGE_DIR) / f\"{image_id}.jpg\"\n",
    "    mask_path = Path(MASK_DIR) / f\"{image_id}.jpg\"\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        img_path = Path(IMAGE_DIR) / f\"{image_id}.png\"\n",
    "    if not mask_path.exists():\n",
    "        mask_path = Path(MASK_DIR) / f\"{image_id}.png\"\n",
    "\n",
    "    if not img_path.exists():\n",
    "        print(f\"Image not found for ID: {image_id}\")\n",
    "        return\n",
    "\n",
    "    # Load data\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    with open(JSON_PATH, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # --- CRITICAL FIX: Access the nested 'bbox' list based on the user's JSON structure ---\n",
    "    image_data = annotations.get(image_id)\n",
    "    if image_data is None:\n",
    "        print(f\"Warning: No annotation entry found for ID: {image_id}\")\n",
    "        return\n",
    "        \n",
    "    boxes = image_data.get('bbox', [])\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    if img is None: return\n",
    "    \n",
    "    # 1. Overlay Mask on Image\n",
    "    \n",
    "    # Resize mask if necessary (Kvasir masks are often 512x512)\n",
    "    if mask is not None and mask.shape[:2] != img.shape[:2]:\n",
    "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert mask to 3 channels for overlay and set transparency\n",
    "    mask_color = np.zeros_like(img, dtype=np.uint8)\n",
    "    mask_color[mask > 127] = [0, 0, 255] # Blue color for mask\n",
    "    \n",
    "    # Blend the mask and image\n",
    "    blended_img = cv2.addWeighted(img, 0.7, mask_color, 0.3, 0)\n",
    "    \n",
    "    # 2. Draw Bounding Boxes\n",
    "    for box_data in boxes:\n",
    "        # box_data is now expected to be a dictionary like {'label': 'polyp', 'xmin': ..., 'ymin': ...}\n",
    "        \n",
    "        # Skip any malformed entries that aren't dictionaries\n",
    "        if not isinstance(box_data, dict):\n",
    "            print(f\"Skipping non-dictionary box data for {image_id}.\")\n",
    "            continue\n",
    "            \n",
    "        # Ensure coordinates are present before accessing\n",
    "        if not all(k in box_data for k in ['xmin', 'ymin', 'xmax', 'ymax']):\n",
    "            print(f\"Skipping box data for {image_id}: Missing coordinate keys.\")\n",
    "            continue\n",
    "            \n",
    "        xmin = int(box_data['xmin'])\n",
    "        ymin = int(box_data['ymin'])\n",
    "        xmax = int(box_data['xmax'])\n",
    "        ymax = int(box_data['ymax'])\n",
    "        \n",
    "        # Draw red bounding box (thickness 3)\n",
    "        cv2.rectangle(blended_img, (xmin, ymin), (xmax, ymax), (255, 0, 0), 3)\n",
    "\n",
    "    # 3. Plotting\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(blended_img)\n",
    "    plt.title(f\"ID: {image_id} | Boxes: {len(boxes)} | Mask Visible: {np.sum(mask>0) > 0}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Starting visualization for problematic IDs...\")\n",
    "print(f\"Total IDs to check: {len(PROBLEM_IDS)}\\n\")\n",
    "\n",
    "for pid in PROBLEM_IDS:\n",
    "    visualize_annotations(pid)\n",
    "\n",
    "print(\"\\nVisualization complete. Review plots to confirm mask-box discrepancy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dab00",
   "metadata": {},
   "source": [
    "PART 2: TRAINING FUNCTION (YOLOv11-seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dbcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_seg(data_yaml_path, model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                   batch_size=BATCH_SIZE, workers=4, lr0=1e-4):\n",
    "    \"\"\"Train YOLOv11 Segmentation model.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 0\n",
    "        print(\"Using NVIDIA GPU (CUDA)\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Load YOLOv11-seg model\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')  # YOLOv11 segmentation\n",
    "\n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=img_size,\n",
    "        batch=batch_size,\n",
    "        name='polyp_segmentation_v11',\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        device=device,\n",
    "        workers=workers,\n",
    "        optimizer='AdamW',\n",
    "        project='Kvasir-mask',\n",
    "        \n",
    "        # Learning rate settings\n",
    "        lr0=lr0,\n",
    "        lrf=0.01,\n",
    "        cos_lr=True,\n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        momentum=0.937,\n",
    "        weight_decay=0.001,\n",
    "        dropout=0.1,\n",
    "        \n",
    "        # Multi-scale training\n",
    "        multi_scale=True,\n",
    "        \n",
    "        # Medical imaging augmentations\n",
    "        mosaic=0.0,          # Disabled for medical\n",
    "        mixup=0.0,           # Light mixup\n",
    "        copy_paste=0.0,      # Copy-paste augmentation\n",
    "        erasing=0.1,         # Random erasing\n",
    "        hsv_h=0.01,          # Minimal hue (preserve color)\n",
    "        hsv_s=0.2,\n",
    "        hsv_v=0.2,\n",
    "        degrees=5.0,\n",
    "        translate=0.05,\n",
    "        scale=0.1,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        shear=1.0,\n",
    "        perspective=0.0001,\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        augment=True,\n",
    "        auto_augment='randaugment',\n",
    "        \n",
    "        # Segmentation specific\n",
    "        mask_ratio=4,\n",
    "        overlap_mask=True\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8c2e7",
   "metadata": {},
   "source": [
    "PART 3: EVALUATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16cec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def normalize_polygon(polygon, img_w, img_h):\n",
    "    \"\"\"Converts normalized YOLO coordinates (0-1) to pixel coordinates.\"\"\"\n",
    "    pixel_coords = []\n",
    "    for i in range(0, len(polygon) - 1, 2):\n",
    "        x = int(polygon[i] * img_w)\n",
    "        y = int(polygon[i+1] * img_h)\n",
    "        pixel_coords.append([x, y])\n",
    "    return np.array(pixel_coords, np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "def add_gt_overlay(image_array, label_path):\n",
    "    \"\"\"Draws ground truth polygons from a YOLO label file onto an image array.\"\"\"\n",
    "    if not label_path.exists():\n",
    "        return image_array\n",
    "        \n",
    "    img_h, img_w, _ = image_array.shape\n",
    "    gt_boxes = []\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                normalized_coords = [float(p) for p in parts[1:]]\n",
    "                gt_boxes.append(normalized_coords)\n",
    "\n",
    "    for gt_polygon in gt_boxes:\n",
    "        pixel_polygon = normalize_polygon(gt_polygon, img_w, img_h)\n",
    "        overlay = image_array.copy()\n",
    "        cv2.fillPoly(overlay, [pixel_polygon], (255, 0, 255))\n",
    "        alpha = 0.35\n",
    "        cv2.addWeighted(overlay, alpha, image_array, 1 - alpha, 0, image_array)\n",
    "        cv2.polylines(image_array, [pixel_polygon], isClosed=True, color=(255, 0, 255), thickness=2)\n",
    "\n",
    "    return image_array\n",
    "\n",
    "def calculate_contingency_counts(precision, recall, total_gt, total_pred):\n",
    "    TP = int(recall * total_gt)  # TP = R × GT\n",
    "    FN = total_gt - TP           # FN = GT - TP\n",
    "    FP = total_pred - TP          # FP = Pred - TP\n",
    "    return TP, FP, FN\n",
    "\n",
    "def evaluate_model_seg(model_path, data_yaml_path, split_name, total_gt, total_pred, conf=0.3, iou=0.5):\n",
    "    \"\"\"Evaluate segmentation model and print comprehensive metrics (Box/Mask P/R/F1 + derived TP/FP/FN).\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    print(f\"\\nRunning validation with NMS IOU = {iou} and Conf = {conf} on split: {split_name}\")\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path, \n",
    "        split=split_name, # Use split_name here\n",
    "        conf=conf,\n",
    "        iou=iou\n",
    "    )\n",
    "\n",
    "    # --- METRIC EXTRACTION ---\n",
    "    \n",
    "    # Extract class 0 metrics (assuming single class)\n",
    "    box_p = metrics.box.p[0] if hasattr(metrics.box, 'p') and len(metrics.box.p) > 0 else 0.0\n",
    "    box_r = metrics.box.r[0] if hasattr(metrics.box, 'r') and len(metrics.box.r) > 0 else 0.0\n",
    "    mask_p = metrics.seg.p[0] if hasattr(metrics.seg, 'p') and len(metrics.seg.p) > 0 else 0.0\n",
    "    mask_r = metrics.seg.r[0] if hasattr(metrics.seg, 'r') and len(metrics.seg.r) > 0 else 0.0\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    box_f1 = 2 * box_p * box_r / (box_p + box_r) if (box_p + box_r) > 0 else 0.0\n",
    "    mask_f1 = 2 * mask_p * mask_r / (mask_p + mask_r) if (mask_p + mask_r) > 0 else 0.0\n",
    "    \n",
    "    # Derive Contingency Counts\n",
    "    TP_box, FP_box, FN_box = calculate_contingency_counts(box_p, box_r, total_gt, total_pred)\n",
    "    TP_mask, FP_mask, FN_mask = calculate_contingency_counts(mask_p, mask_r, total_gt, total_pred)\n",
    "    \n",
    "    # --- PRINT SUMMARY ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION RESULTS ON {split_name.upper()} SET (Polyp-Level Metrics)\")\n",
    "    print(f\"Total Ground Truth Polyps: {total_gt} | Total Predicted Polyps: {total_pred}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Box metrics (Polyp-Level Detection)\n",
    "    print(\"\\n### Box (Detection) Metrics (Polyp-Level Contingency via IoU)\")\n",
    "    print(f\"Box Contingency (TP/FP/FN): {TP_box}/{FP_box}/{FN_box}\")\n",
    "    print(f\"Box mAP@0.50     : {metrics.box.map50:.4f}\")\n",
    "    print(f\"Box Precision    : {box_p:.4f}\")\n",
    "    print(f\"Box Recall       : {box_r:.4f}\")\n",
    "    print(f\"Box F1-score     : {box_f1:.4f}\")\n",
    "    \n",
    "    # Mask metrics (Polyp-Level Segmentation)\n",
    "    print(\"\\n### Mask (Segmentation) Metrics (Polyp-Level Contingency via IoU)\")\n",
    "    print(f\"Mask Contingency (TP/FP/FN): {TP_mask}/{FP_mask}/{FN_mask}\")\n",
    "    print(f\"Mask mAP@0.50    : {metrics.seg.map50:.4f}\")\n",
    "    print(f\"Mask Precision   : {mask_p:.4f}\")\n",
    "    print(f\"Mask Recall      : {mask_r:.4f}\")\n",
    "    print(f\"Mask F1-score    : {mask_f1:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def predict_on_all_images_seg(model_path, image_dir, data_yaml_path=None, \n",
    "                               conf_threshold=0.25, save_dir='predictions_seg', iou=0.5):\n",
    "    \"\"\"\n",
    "    Run inference, save predicted images with GT overlay, and compute statistics.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)\n",
    "    image_dir = Path(image_dir)\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SETUP PATHS ---\n",
    "    label_dir = image_dir.parent.parent / 'labels' / image_dir.name\n",
    "    image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "\n",
    "    # --- INITIALIZE COUNTERS ---\n",
    "    total_gt_polyps = 0\n",
    "    total_pred_polyps = 0\n",
    "    TP_img = FP_img = FN_img = TN_img = 0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"1. RUNNING INFERENCE AND VISUALIZATION ON {len(image_files)} IMAGES (Saving GT Overlay)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # --- 1. INFERENCE, VISUALIZATION, AND IMAGE-LEVEL COUNTING LOOP ---\n",
    "    for img_path in image_files:\n",
    "        label_path = label_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        # Check ground truth\n",
    "        gt_boxes = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                gt_boxes = [line for line in f if line.strip()]\n",
    "        num_gt = len(gt_boxes)\n",
    "        total_gt_polyps += num_gt\n",
    "\n",
    "        # Run inference (single image)\n",
    "        results = model(str(img_path), conf=conf_threshold, iou=iou, verbose=False)\n",
    "        result = results[0] \n",
    "        \n",
    "        # Save image with boxes AND masks\n",
    "        # 1. Get image array with PREDICTED results plotted\n",
    "        img_with_results = result.plot()\n",
    "        \n",
    "        # 2. ADD GROUND TRUTH OVERLAY \n",
    "        img_with_gt_overlay = add_gt_overlay(img_with_results, label_path)\n",
    "        \n",
    "        # 3. Save the final image\n",
    "        output_path = save_dir / f\"pred_gt_{img_path.name}\"\n",
    "        img_to_save = np.ascontiguousarray(img_with_gt_overlay, dtype=np.uint8)\n",
    "        cv2.imwrite(str(output_path), img_to_save)\n",
    "\n",
    "        # Count predictions\n",
    "        num_pred = 0\n",
    "        if hasattr(result, 'masks') and result.masks is not None:\n",
    "            num_pred = len(result.masks)\n",
    "        elif hasattr(result, 'boxes') and result.boxes.shape[0] > 0:\n",
    "             num_pred = result.boxes.shape[0]\n",
    "\n",
    "        total_pred_polyps += num_pred\n",
    "        \n",
    "        # Contingency matrix (image-level)\n",
    "        if num_gt > 0 and num_pred > 0:\n",
    "            TP_img += 1\n",
    "        elif num_gt == 0 and num_pred > 0:\n",
    "            FP_img += 1\n",
    "        elif num_gt > 0 and num_pred == 0:\n",
    "            FN_img += 1\n",
    "        elif num_gt == 0 and num_pred == 0:\n",
    "            TN_img += 1\n",
    "            \n",
    "        print(f\"  {img_path.name}: GT={num_gt}, Pred={num_pred}\")\n",
    "\n",
    "    # --- 2. OBJECT-LEVEL METRIC CALCULATION (mAP and Polyp-Level) ---\n",
    "    \n",
    "    # Calculate Image-Level P/R/F1\n",
    "    precision_img = TP_img / (TP_img + FP_img) if (TP_img + FP_img) > 0 else 0.0\n",
    "    recall_img = TP_img / (TP_img + FN_img) if (TP_img + FN_img) > 0 else 0.0\n",
    "    f1_score_img = 2 * precision_img * recall_img / (precision_img + recall_img) if (precision_img + recall_img) > 0 else 0.0\n",
    "\n",
    "    # --- 3. FINAL SUMMARY (MODIFIED) ---\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"Total images:              {len(image_files)}\")\n",
    "    print(f\"Total ground truth polyps: {total_gt_polyps}\")\n",
    "    print(f\"Total predicted polyps:    {total_pred_polyps}\")\n",
    "    \n",
    "    # Image-Level Metrics\n",
    "    print(f\"\\n### Image-Level Metrics (Detection/No-Detection)\")\n",
    "    print(f\"Contingency Matrix (TP/FP/FN/TN): {TP_img}/{FP_img}/{FN_img}/{TN_img}\")\n",
    "    print(f\"  Precision: {precision_img:.4f}\")\n",
    "    print(f\"  Recall:    {recall_img:.4f}\")\n",
    "    print(f\"  F1-score:  {f1_score_img:.4f}\")\n",
    "\n",
    "    # Polyp-Level Metrics (using the dedicated function)\n",
    "    if data_yaml_path:\n",
    "        # Pass the total polyp counts to the evaluation function for derived contingency\n",
    "        metrics = evaluate_model_seg(model_path, data_yaml_path, split_name=image_dir.name, \n",
    "                                     total_gt=total_gt_polyps, total_pred=total_pred_polyps,\n",
    "                                     conf=conf_threshold, iou=iou)\n",
    "    else:\n",
    "        print(\"\\nNote: 'data_yaml_path' not provided. Cannot compute robust polyp-level metrics.\")\n",
    "\n",
    "    print(f\"\\nPredictions saved with GT overlay to: {save_dir.resolve()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Return the metrics of the mask segmentation\n",
    "    if data_yaml_path and hasattr(metrics, 'seg'):\n",
    "        mask_p = metrics.seg.p[0] if len(metrics.seg.p) > 0 else 0.0\n",
    "        mask_r = metrics.seg.r[0] if len(metrics.seg.r) > 0 else 0.0\n",
    "        mask_f1 = 2 * mask_p * mask_r / (mask_p + mask_r) if (mask_p + mask_r) > 0 else 0.0\n",
    "        return mask_p, mask_r, mask_f1, metrics.seg.map50, metrics.seg.map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03825d20",
   "metadata": {},
   "source": [
    "Run the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2da8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: DATASET PREPARATION (SEGMENTATION)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DATASET SPLIT\n",
      "Train: 1050 (700 polyps + 350 healthy)\n",
      "Val:   300 (200 polyps + 100 healthy)\n",
      "Test:  150 (100 polyps + 50 healthy)\n",
      "======================================================================\n",
      "\n",
      "Processing train split (1050 images)...\n",
      "Processing val split (300 images)...\n",
      "Processing test split (150 images)...\n",
      "\n",
      "  Created: Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\n",
      "\n",
      "✓ Segmentation dataset created!\n",
      "  Output: /home/luca/Desktop/Luca/File-di-kvasir_Daniele/Kvasir-mask/kvasir_yolo_seg_dataset\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# ========== STEP 1: Dataset Preparation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATASET PREPARATION (SEGMENTATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "converter = KvasirToYOLOSeg(IMAGE_DIR, MASK_DIR, JSON_PATH, OUTPUT_DIR, seed=SEED)\n",
    "converter.prepare_dataset(train_split=0.7, val_split=0.2, test_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(f'yolo11{MODEL_SIZE}-seg.pt')  # Replace with your model path if different\n",
    "\n",
    "search_space = {\n",
    "    \"lr0\": (1e-5, 1e-3),\n",
    "    \"lrf\": (0.01, 0.1),\n",
    "    \"momentum\": (0.6, 0.98),\n",
    "    \"weight_decay\": (0.0, 0.001),\n",
    "    \"dfl\": (1.0, 2.0),\n",
    "    \"hsv_h\": (0.0, 0.02),\n",
    "    \"hsv_s\": (0.0, 0.3),\n",
    "    \"hsv_v\": (0.0, 0.3),\n",
    "    \"degrees\": (0.0, 10.0),\n",
    "    \"translate\": (0.0, 0.1),\n",
    "    \"scale\": (0.0, 0.2),\n",
    "    \"shear\": (0.0, 2.0),\n",
    "    \"perspective\": (0.0, 0.0001),\n",
    "    \"flipud\": (0.0, 1.0),\n",
    "    \"fliplr\": (0.0, 1.0),\n",
    "    \"box\": (3.0, 7.5),     # Box loss weight\n",
    "    \"cls\": (0.2, 2.0) \n",
    "}\n",
    "\n",
    "model.tune(\n",
    "    data=\"Kvasir-mask/kvasir_yolo_seg_dataset/data.yaml\",\n",
    "    epochs=10,\n",
    "    iterations=100,\n",
    "    optimizer=\"AdamW\",\n",
    "    space=search_space,\n",
    "    plots=True,\n",
    "    save=True,\n",
    "    val=True,\n",
    "    project=\"Kvasir-mask/tune\"  # ✅ Saves results in ./Kvasir_mask/tune/\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "    \"\"\"Search for best_hyperparameters.yaml in tune directory.\"\"\"\n",
    "    tune_dir = Path(\"Kvasir-mask/tune\")\n",
    "    \n",
    "    if not tune_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Search recursively for the file\n",
    "    for yaml_file in tune_dir.rglob(\"best_hyperparameters.yaml\"):\n",
    "        print(f\"Found hyperparameters at: {yaml_file}\")\n",
    "        return yaml_file\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def train_yolo_seg_with_tuned_params(data_yaml_path, best_hyperparameters, \n",
    "                                      model_size=MODEL_SIZE, epochs=100, img_size=640, \n",
    "                                      batch_size=BATCH_SIZE, workers=4):\n",
    "    \"\"\"Train with tuned hyperparameters.\"\"\"\n",
    "    \n",
    "    # Device detection\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    model = YOLO(f'yolo11{model_size}-seg.pt')\n",
    "\n",
    "    # ✅ Merge your fixed settings with tuned parameters\n",
    "    training_args = {\n",
    "        'data': data_yaml_path,\n",
    "        'epochs': epochs,\n",
    "        'imgsz': img_size,\n",
    "        'batch': batch_size,\n",
    "        'name': 'polyp_segmentation_v11_tuned',\n",
    "        'patience': 10,\n",
    "        'save': True,\n",
    "        'device': device,\n",
    "        'workers': workers,\n",
    "        'optimizer': 'AdamW',\n",
    "        'project': 'Kvasir-mask',\n",
    "        \n",
    "        # Fixed settings (always use these)\n",
    "        'multi_scale': True,\n",
    "        'mosaic': 0.0,\n",
    "        'mixup': 0.0,\n",
    "        'copy_paste': 0.0,\n",
    "        'augment': True,\n",
    "        'auto_augment': 'randaugment',\n",
    "        'mask_ratio': 4,\n",
    "        'overlap_mask': True,\n",
    "        \n",
    "        # ✅ Add tuned hyperparameters (these override defaults)\n",
    "        **best_hyperparameters  # This unpacks the dictionary\n",
    "    }\n",
    "\n",
    "    results = model.train(**training_args)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: Train YOLOv11-seg ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAINING YOLOv11-seg MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tuned_params_path = find_best_hyperparameters()\n",
    "\n",
    "if tuned_params_path and tuned_params_path.exists():\n",
    "    print(f\"\\n✓ Found tuned hyperparameters at: {tuned_params_path}\")\n",
    "    \n",
    "    with open(tuned_params_path, 'r') as f:\n",
    "        best_hyperparameters = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"\\n📋 Loaded Hyperparameters:\")\n",
    "    for key, value in best_hyperparameters.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Train with tuned parameters\n",
    "    model = train_yolo_seg_with_tuned_params(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        best_hyperparameters=best_hyperparameters,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11_tuned/weights/best.pt\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No tuned parameters found, using defaults...\")\n",
    "    model = train_yolo_seg(\n",
    "        data_yaml_path=DATA_YAML,\n",
    "        model_size=MODEL_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        img_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr0=1e-4  # Good starting point for nano\n",
    "    )\n",
    "    BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11/weights/best.pt\")\n",
    "\n",
    "print(f\"\\n✓ Using model: {BEST_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69861181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4B: RUNNING INFERENCE ON ALL TEST IMAGES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "1. RUNNING INFERENCE AND VISUALIZATION ON 150 IMAGES (Saving GT Overlay)\n",
      "======================================================================\n",
      "\n",
      "  4ead1eda-9550-4245-8505-10a9134ad08d.jpg: GT=0, Pred=0\n",
      "  38591735-1d34-4f14-b900-10f889d3f0f4.jpg: GT=1, Pred=1\n",
      "  57d5d73c-7cb3-488d-8bbd-b0be40dae539.jpg: GT=1, Pred=1\n",
      "  ff750221-36bc-44b6-9b73-2657debf4baa.jpg: GT=1, Pred=1\n",
      "  c4a13518-1519-4579-aa66-c689de9d3f47.jpg: GT=1, Pred=1\n",
      "  54d45c57-2392-4b22-a8ba-5c06f21bea8a.jpg: GT=1, Pred=1\n",
      "  67d81f62-08c3-44ce-9a85-dc2fbe6da942.jpg: GT=0, Pred=0\n",
      "  84461c07-b5ff-4602-9cf7-6dbe0ed178f0.jpg: GT=0, Pred=0\n",
      "  96b2a75d-95f1-4336-a30e-600426da2551.jpg: GT=1, Pred=1\n",
      "  5cb6d900-c930-4094-91ca-ccff14a2e176.jpg: GT=0, Pred=0\n",
      "  1d68aeba-4fdd-42da-9d10-c65d24f60ea6.jpg: GT=0, Pred=0\n",
      "  d05ef2f2-603d-432f-b7fb-edf0101967bd.jpg: GT=1, Pred=1\n",
      "  c0941c98-2dc9-434d-bb30-c42cacc00f8c.jpg: GT=1, Pred=1\n",
      "  66bc7b09-920d-4c91-ad84-5f4586ce6707.jpg: GT=1, Pred=1\n",
      "  dc748fe5-0b3a-41bc-a68c-d9a9b70e034e.jpg: GT=1, Pred=1\n",
      "  832c8d87-0a93-4c60-82b8-9192be2f1663.jpg: GT=1, Pred=1\n",
      "  0e2257ff-dab7-405b-a089-0ac5b6ec6731.jpg: GT=1, Pred=1\n",
      "  f9be60f7-fa95-47d7-a18d-5cae92ac7279.jpg: GT=1, Pred=1\n",
      "  1ac61eb3-2323-438c-80c0-41aef0957e60.jpg: GT=0, Pred=0\n",
      "  08a43621-f3b8-493a-978d-0c802f1cad82.jpg: GT=1, Pred=1\n",
      "  8ea05ad6-2986-42af-a856-ccc3ac9d704c.jpg: GT=0, Pred=0\n",
      "  88d5fa79-cbca-46d3-9155-9f3cd0e3822f.jpg: GT=1, Pred=1\n",
      "  633d1ea5-6a3d-4227-9662-a3af01e2847b.jpg: GT=1, Pred=1\n",
      "  93f8dcbb-a5e0-4b6e-9a29-7fad19d04e23.jpg: GT=1, Pred=1\n",
      "  3cf2b3d3-5fbd-488e-94e1-68bc29936a59.jpg: GT=0, Pred=0\n",
      "  98e8854c-d43f-4cc4-bf8e-575c37a5b48b.jpg: GT=1, Pred=1\n",
      "  03b32e65-b829-4257-8eb1-b3f3c224f3ac.jpg: GT=0, Pred=0\n",
      "  1d98fd36-75ad-4213-ab41-b9f0098cdcec.jpg: GT=1, Pred=1\n",
      "  60841dc1-8e33-43c0-85cc-353d2c2bf37d.jpg: GT=1, Pred=1\n",
      "  d8c3f635-1970-4954-b0de-ba58c74be2c9.jpg: GT=0, Pred=0\n",
      "  0677fde3-e2a1-4b1f-be82-f8328caef75e.jpg: GT=0, Pred=0\n",
      "  eb555b8a-67bf-4a69-817a-e5d10b168ea5.jpg: GT=1, Pred=1\n",
      "  737f6610-4a35-4092-81a9-2c9af4262954.jpg: GT=2, Pred=2\n",
      "  e573fe87-bec4-4af7-b20c-a8b177c2b461.jpg: GT=1, Pred=1\n",
      "  7ca1511a-5695-4868-9757-718e81728e66.jpg: GT=1, Pred=1\n",
      "  6b4061ed-b6a5-44ac-8408-5b9abb80f461.jpg: GT=1, Pred=1\n",
      "  e4381ce7-637b-456e-9670-462eed63c948.jpg: GT=1, Pred=1\n",
      "  4016dab7-abd0-42a2-bb51-69c5b617a263.jpg: GT=0, Pred=0\n",
      "  ee686e22-85ad-431c-99cf-5dc134230dd1.jpg: GT=1, Pred=1\n",
      "  4f0716ce-3bf9-4f43-ab27-a0e75b6bc64b.jpg: GT=0, Pred=0\n",
      "  f50a5e64-0bb3-4885-b751-11a7a7174970.jpg: GT=1, Pred=1\n",
      "  fb02fe59-68de-469b-a4fd-d4ccea02233a.jpg: GT=1, Pred=1\n",
      "  52893e8c-1e88-4069-8831-c65551c4fc57.jpg: GT=1, Pred=1\n",
      "  540664cb-d9c9-47f7-92ec-1b6efed571f3.jpg: GT=1, Pred=1\n",
      "  ad4bf005-5ef1-449a-8b27-bcc7d63ebc68.jpg: GT=1, Pred=1\n",
      "  21f2237a-6e54-46c5-b1e0-92384044a018.jpg: GT=0, Pred=0\n",
      "  5fcaa233-af5f-41c5-86ec-9f2864fc2c27.jpg: GT=0, Pred=0\n",
      "  639d7e51-7411-4413-ba53-13b73e932f16.jpg: GT=1, Pred=1\n",
      "  a8426edc-4ca4-4199-8a2b-09ea3ed0a83f.jpg: GT=1, Pred=1\n",
      "  44d19905-4686-4778-966b-8795ab90cd37.jpg: GT=1, Pred=1\n",
      "  4c53aa4c-8328-444a-b8db-e0eae6bf4bbb.jpg: GT=0, Pred=0\n",
      "  41d257fa-6f7f-4638-b2e7-c7baf002f973.jpg: GT=0, Pred=0\n",
      "  33ef43c7-c279-4d6f-9d87-4148f1e609cc.jpg: GT=0, Pred=0\n",
      "  041b7e2c-e6ed-4da0-8e7d-f3d04483d7ed.jpg: GT=0, Pred=0\n",
      "  77ddd87e-1790-47c9-9c63-bbb74f532dbc.jpg: GT=1, Pred=1\n",
      "  4604cf0d-7992-48a4-baba-51768bcf0469.jpg: GT=10, Pred=3\n",
      "  21c1ac73-7e82-4e00-858f-72896beec9be.jpg: GT=0, Pred=0\n",
      "  934bc960-3f14-4dae-879f-1248249b3efb.jpg: GT=0, Pred=0\n",
      "  b74cd753-f5fa-489a-9395-6046a6c04b79.jpg: GT=1, Pred=1\n",
      "  d9e12995-94e5-45f1-930a-343aa2521d0d.jpg: GT=1, Pred=1\n",
      "  9dbd571f-1012-4a6e-b05e-773ae4fe5896.jpg: GT=0, Pred=0\n",
      "  20ea2b2f-752a-436f-9a52-5a3a678d28ac.jpg: GT=1, Pred=1\n",
      "  6406e362-538d-44bc-aebe-9bb90f8930d7.jpg: GT=1, Pred=1\n",
      "  1e9efe72-cb72-4b4d-a119-7d2a035635c3.jpg: GT=0, Pred=0\n",
      "  eba1e589-8f32-44bb-8e17-d85a282e5cfe.jpg: GT=1, Pred=1\n",
      "  3dae0102-2c3c-4c87-a080-4d0c5df6412b.jpg: GT=1, Pred=1\n",
      "  ebecb670-0071-4a53-ae55-bc1df538e0ca.jpg: GT=1, Pred=1\n",
      "  89a79812-a327-477d-aa2c-dbc8c22bc9af.jpg: GT=1, Pred=1\n",
      "  10a19193-67e3-41fa-899b-cd7dc5b64274.jpg: GT=0, Pred=0\n",
      "  d17f72d7-97b7-4d24-918e-24e4924827de.jpg: GT=1, Pred=1\n",
      "  0cac37d3-0eec-44c7-af10-c88dc8549cbe.jpg: GT=1, Pred=1\n",
      "  1a5e459b-f8e2-43be-8a29-63e932497cfc.jpg: GT=0, Pred=0\n",
      "  1b3fc90d-c56a-4785-a08e-2dfff89b0f11.jpg: GT=0, Pred=0\n",
      "  419b95af-cd80-4fb5-95d3-55aa3d223a6b.jpg: GT=1, Pred=1\n",
      "  5b9589d9-78ac-4d7c-9862-4d40bdc14c16.jpg: GT=1, Pred=1\n",
      "  01c95ee8-79cc-46f4-a6a0-834d8a6045d4.jpg: GT=1, Pred=1\n",
      "  7ba25e62-95ef-4dcf-812d-b81c6edd2f96.jpg: GT=0, Pred=0\n",
      "  17b2c0c7-a60d-4bb1-ab83-6847fdff6f12.jpg: GT=0, Pred=0\n",
      "  c783b10b-e4ba-4ee4-9094-786ff0b6fa84.jpg: GT=1, Pred=1\n",
      "  b3220fae-52ee-493c-aebc-cfd6455be908.jpg: GT=2, Pred=2\n",
      "  6a0754b2-728d-432b-ab4f-9b89fff2ae1d.jpg: GT=0, Pred=0\n",
      "  455d45c7-7847-439e-a4a8-461ff2dad4fe.jpg: GT=0, Pred=0\n",
      "  e441325d-73d6-4a3e-9193-eac98f05f7fa.jpg: GT=2, Pred=2\n",
      "  760e8f16-a086-49d8-bc07-10715933ba50.jpg: GT=1, Pred=1\n",
      "  fca927d3-55d2-4be7-9f45-7cb70ca41d1d.jpg: GT=1, Pred=1\n",
      "  ead9c47b-cd35-4a1b-82d4-ab5602af8d02.jpg: GT=0, Pred=0\n",
      "  c4382db9-1b32-4247-800d-9c30d6631e64.jpg: GT=1, Pred=1\n",
      "  7e340833-b661-4357-a099-b8cd65c5e3f5.jpg: GT=1, Pred=1\n",
      "  0dad158b-043d-427d-a656-b1034b9a6a0d.jpg: GT=1, Pred=1\n",
      "  49ec1434-c021-4f11-8c94-356ef41f8abf.jpg: GT=1, Pred=1\n",
      "  cbadc5af-e028-4a62-9264-148bd3035958.jpg: GT=1, Pred=1\n",
      "  683c2b9b-7e3a-4331-b833-74010b8e81f7.jpg: GT=0, Pred=0\n",
      "  c5f46088-a8b5-4db8-8e61-51f0f3e2dd18.jpg: GT=1, Pred=1\n",
      "  9035c364-38ac-4298-a8c3-54b393e7d4c0.jpg: GT=1, Pred=1\n",
      "  1020f3b4-12fa-406c-aa42-83d75d221005.jpg: GT=1, Pred=1\n",
      "  8b7e4f1e-5266-4fff-9563-9309e104dcd4.jpg: GT=0, Pred=0\n",
      "  e58fdb27-3c54-4bdb-bcc9-05be9d5d0092.jpg: GT=1, Pred=1\n",
      "  88b4ae69-1ca3-47df-bb1d-a0a6c48110a9.jpg: GT=0, Pred=0\n",
      "  0017b7c7-90f8-4de2-8723-1d87e5c58317.jpg: GT=3, Pred=3\n",
      "  bceac4e5-97b5-4a39-b4bf-b99c5c697035.jpg: GT=1, Pred=1\n",
      "  19eb0a40-b47a-4dad-9241-99b6641e4110.jpg: GT=1, Pred=1\n",
      "  82e9f319-447d-4451-a500-2d345c7f531f.jpg: GT=1, Pred=1\n",
      "  5166c1a5-cb31-453d-a319-d30c80d9b78f.jpg: GT=1, Pred=1\n",
      "  605b01a7-9e3f-4b03-8ac1-43fe627c4c82.jpg: GT=2, Pred=2\n",
      "  8fc70ff3-f538-49ab-b153-e3315e11c913.jpg: GT=2, Pred=2\n",
      "  67deb716-4d5e-4256-84d5-229a18e5873e.jpg: GT=1, Pred=1\n",
      "  606e8cb6-b60b-490b-8f4e-df59279dafca.jpg: GT=1, Pred=1\n",
      "  778749d9-1afb-4239-be32-fd81ee8b45e1.jpg: GT=1, Pred=1\n",
      "  a1f003f0-d813-483c-aaef-96b3bafa7fd1.jpg: GT=1, Pred=1\n",
      "  44ff1c6a-fa66-43e1-b622-c48ebde75307.jpg: GT=1, Pred=1\n",
      "  1cd92a82-81a8-44a8-ba35-4c9669b99c37.jpg: GT=1, Pred=1\n",
      "  18b7ab44-17b7-48c8-b400-44ca5b2574da.jpg: GT=1, Pred=1\n",
      "  60508423-0519-4b55-97a4-e4f2a8446e3f.jpg: GT=1, Pred=1\n",
      "  d83c046e-918d-450b-a204-be94c98c3ccf.jpg: GT=1, Pred=1\n",
      "  77a368aa-8363-4709-bd7b-6f50be722b8a.jpg: GT=1, Pred=1\n",
      "  0cf8e803-870d-40a1-8e8a-8c4ded51561e.jpg: GT=0, Pred=0\n",
      "  dea32034-3795-463c-86dd-12fbb107695b.jpg: GT=1, Pred=1\n",
      "  53067fc1-44d0-4023-a2a9-af6b31b4c3f8.jpg: GT=0, Pred=0\n",
      "  5ac91d24-d9d5-4a51-8169-7eebef02ab2f.jpg: GT=0, Pred=0\n",
      "  5fea2d4a-d2e0-4d8d-90c7-57d6ddab1dd7.jpg: GT=1, Pred=1\n",
      "  ac61069a-7b61-4cf8-b2e4-20395b717f0d.jpg: GT=1, Pred=1\n",
      "  1033b8ec-9777-4add-b023-b884df880de5.jpg: GT=1, Pred=1\n",
      "  8f20d83d-d459-4035-a5ef-e7aae9cce960.jpg: GT=1, Pred=1\n",
      "  14552594-a205-4c89-a9e9-ee8ee07d4f7e.jpg: GT=1, Pred=1\n",
      "  fd9d587c-98d1-46f8-b8cb-3f9f61931656.jpg: GT=2, Pred=2\n",
      "  955c65a5-47ea-4972-8cac-e3a63a7ba760.jpg: GT=1, Pred=1\n",
      "  843a247c-c448-4559-b56c-7658781e6a01.jpg: GT=1, Pred=1\n",
      "  f845295e-6b54-4677-bc6b-3b7756d67106.jpg: GT=1, Pred=1\n",
      "  312ac427-4fb6-458f-b1bb-06b06ad8b54f.jpg: GT=2, Pred=1\n",
      "  a8105310-738e-4eef-ac6c-f17adb93205e.jpg: GT=1, Pred=1\n",
      "  87bfeba2-737e-4d0a-a499-3e9ef35b321f.jpg: GT=1, Pred=1\n",
      "  8b9f345f-ab6f-4dae-82f5-ded09e40cc81.jpg: GT=0, Pred=0\n",
      "  48a95760-d98a-4dd6-bf5d-32b94c01187e.jpg: GT=0, Pred=0\n",
      "  831db0a8-17a5-4d31-ba13-135c81a46b25.jpg: GT=0, Pred=0\n",
      "  22b6ea88-9236-4d51-91a8-d6257d42035a.jpg: GT=1, Pred=1\n",
      "  25cebc87-8de9-4b33-b7d3-94d12d134b6c.jpg: GT=0, Pred=0\n",
      "  420e19d8-69d2-4157-a6ca-656206886cde.jpg: GT=0, Pred=0\n",
      "  3f7c1c00-6f4d-42dc-a97a-e36e04037664.jpg: GT=0, Pred=0\n",
      "  98d1679d-ff59-4aad-9f69-2e1780de7de6.jpg: GT=2, Pred=2\n",
      "  be2b6b44-d5dc-44ce-a0de-bdbe0a88068a.jpg: GT=1, Pred=1\n",
      "  d245907f-a004-49f9-9fcc-8022b13abdc9.jpg: GT=0, Pred=0\n",
      "  914bf111-bc57-4a52-87f7-cc6ed77fbc04.jpg: GT=0, Pred=0\n",
      "  9bdda2b3-df46-441b-9675-8d8a92789c7d.jpg: GT=0, Pred=0\n",
      "  0048d8c5-b59d-461c-9834-f44a727e191d.jpg: GT=1, Pred=1\n",
      "  1f055f84-7ee0-4109-b149-107abb1dd1f2.jpg: GT=1, Pred=1\n",
      "  2029d77f-86c4-49ee-bfe8-352d7deb171e.jpg: GT=0, Pred=0\n",
      "  2f728302-8ed7-429f-9c46-8e203b64e438.jpg: GT=1, Pred=1\n",
      "  e667ee83-4435-42c6-b69c-c9618dda2aa5.jpg: GT=0, Pred=0\n",
      "  704b2f84-befc-4231-a73a-cae434866b4e.jpg: GT=0, Pred=0\n",
      "  17b277d1-ffa1-4b77-abc1-75503bea4330.jpg: GT=0, Pred=0\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Total images:              150\n",
      "Total ground truth polyps: 119\n",
      "Total predicted polyps:    111\n",
      "\n",
      "### Image-Level Metrics (Detection/No-Detection)\n",
      "Contingency Matrix (TP/FP/FN/TN): 100/0/0/50\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "\n",
      "Running validation with NMS IOU = 0.5 and Conf = 0.25 on split: test\n",
      "Ultralytics 8.3.223 🚀 Python-3.12.3 torch-2.9.0+cu128 CPU (AMD Ryzen 5 5600X 6-Core Processor)\n",
      "YOLO11m-seg summary (fused): 138 layers, 22,336,083 parameters, 0 gradients, 112.9 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2266.4±983.1 MB/s, size: 57.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/luca/Desktop/Luca/File-di-kvasir_Daniele/Kvasir-mask/kvasir_yolo_seg_dataset/labels/test... 150 images, 50 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 150/150 2.2Kit/s 0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/luca/Desktop/Luca/File-di-kvasir_Daniele/Kvasir-mask/kvasir_yolo_seg_dataset/labels/test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 10/10 0.2it/s 1:03.2s3s\n",
      "                   all        150        119      0.991      0.916      0.957      0.895      0.991      0.916      0.957      0.874\n",
      "Speed: 1.7ms preprocess, 413.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/luca/Desktop/Luca/File-di-kvasir_Daniele/runs/segment/val12\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "EVALUATION RESULTS ON TEST SET (Polyp-Level Metrics)\n",
      "Total Ground Truth Polyps: 119 | Total Predicted Polyps: 111\n",
      "======================================================================\n",
      "\n",
      "### Box (Detection) Metrics (Polyp-Level Contingency via IoU)\n",
      "Box Contingency (TP/FP/FN): 109/2/10\n",
      "Box mAP@0.50     : 0.9566\n",
      "Box Precision    : 0.9909\n",
      "Box Recall       : 0.9160\n",
      "Box F1-score     : 0.9519\n",
      "\n",
      "### Mask (Segmentation) Metrics (Polyp-Level Contingency via IoU)\n",
      "Mask Contingency (TP/FP/FN): 109/2/10\n",
      "Mask mAP@0.50    : 0.9566\n",
      "Mask Precision   : 0.9909\n",
      "Mask Recall      : 0.9160\n",
      "Mask F1-score    : 0.9519\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Predictions saved with GT overlay to: /home/luca/Desktop/Luca/File-di-kvasir_Daniele/test_predictions_seg\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "✅ PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_PATH = Path(\"Kvasir-mask/polyp_segmentation_v11_tuned/weights/best.pt\")\n",
    "\n",
    "# ========== Full Test Set Evaluation ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4B: RUNNING INFERENCE ON ALL TEST IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predict_on_all_images_seg(\n",
    "    BEST_MODEL_PATH, \n",
    "    f\"{OUTPUT_DIR}/images/test\",\n",
    "    data_yaml_path=DATA_YAML,\n",
    "    conf_threshold=0.25,\n",
    "    iou=0.5,#increase to be more lax\n",
    "    save_dir='test_predictions_seg'\n",
    ")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
